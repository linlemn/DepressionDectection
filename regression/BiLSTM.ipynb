{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 1024)\n",
      "(49, 1024)\n",
      "(46, 1024)\n",
      "(47, 1024)\n",
      "(42, 1024)\n",
      "(43, 1024)\n",
      "(48, 1024)\n",
      "(55, 1024)\n",
      "(50, 1024)\n",
      "(38, 1024)\n",
      "(44, 1024)\n",
      "(63, 1024)\n",
      "(59, 1024)\n",
      "(54, 1024)\n",
      "(44, 1024)\n",
      "(38, 1024)\n",
      "(56, 1024)\n",
      "(55, 1024)\n",
      "(41, 1024)\n",
      "(48, 1024)\n",
      "(46, 1024)\n",
      "(56, 1024)\n",
      "(41, 1024)\n",
      "(49, 1024)\n",
      "(48, 1024)\n",
      "(47, 1024)\n",
      "(63, 1024)\n",
      "(55, 1024)\n",
      "(37, 1024)\n",
      "(50, 1024)\n",
      "(56, 1024)\n",
      "(44, 1024)\n",
      "(51, 1024)\n",
      "(32, 1024)\n",
      "(43, 1024)\n",
      "(38, 1024)\n",
      "(37, 1024)\n",
      "(30, 1024)\n",
      "(41, 1024)\n",
      "(33, 1024)\n",
      "(42, 1024)\n",
      "(33, 1024)\n",
      "(42, 1024)\n",
      "(34, 1024)\n",
      "(28, 1024)\n",
      "(31, 1024)\n",
      "(25, 1024)\n",
      "(54, 1024)\n",
      "(43, 1024)\n",
      "(39, 1024)\n",
      "(46, 1024)\n",
      "(40, 1024)\n",
      "(35, 1024)\n",
      "(38, 1024)\n",
      "(34, 1024)\n",
      "(48, 1024)\n",
      "(42, 1024)\n",
      "(47, 1024)\n",
      "(43, 1024)\n",
      "(39, 1024)\n",
      "(54, 1024)\n",
      "(53, 1024)\n",
      "(43, 1024)\n",
      "(41, 1024)\n",
      "(29, 1024)\n",
      "(53, 1024)\n",
      "(35, 1024)\n",
      "(44, 1024)\n",
      "(51, 1024)\n",
      "(37, 1024)\n",
      "(51, 1024)\n",
      "(40, 1024)\n",
      "(48, 1024)\n",
      "(47, 1024)\n",
      "(48, 1024)\n",
      "(51, 1024)\n",
      "(46, 1024)\n",
      "(47, 1024)\n",
      "(37, 1024)\n",
      "(44, 1024)\n",
      "(68, 1024)\n",
      "(52, 1024)\n",
      "(37, 1024)\n",
      "(43, 1024)\n",
      "(34, 1024)\n",
      "(38, 1024)\n",
      "(32, 1024)\n",
      "(33, 1024)\n",
      "(64, 1024)\n",
      "(34, 1024)\n",
      "(48, 1024)\n",
      "(47, 1024)\n",
      "(59, 1024)\n",
      "(48, 1024)\n",
      "(40, 1024)\n",
      "(35, 1024)\n",
      "(42, 1024)\n",
      "(54, 1024)\n",
      "(45, 1024)\n",
      "(37, 1024)\n",
      "(49, 1024)\n",
      "(54, 1024)\n",
      "(48, 1024)\n",
      "(43, 1024)\n",
      "(35, 1024)\n",
      "(35, 1024)\n",
      "(50, 1024)\n",
      "(38, 1024)\n",
      "(35, 1024)\n",
      "(42, 1024)\n",
      "(46, 1024)\n",
      "(42, 1024)\n",
      "(31, 1024)\n",
      "(43, 1024)\n",
      "(45, 1024)\n",
      "(47, 1024)\n",
      "(45, 1024)\n",
      "(62, 1024)\n",
      "(48, 1024)\n",
      "(46, 1024)\n",
      "(37, 1024)\n",
      "(43, 1024)\n",
      "(39, 1024)\n",
      "(41, 1024)\n",
      "(38, 1024)\n",
      "(42, 1024)\n",
      "(55, 1024)\n",
      "(33, 1024)\n",
      "(42, 1024)\n",
      "(33, 1024)\n",
      "(51, 1024)\n",
      "(30, 1024)\n",
      "(40, 1024)\n",
      "(43, 1024)\n",
      "(44, 1024)\n",
      "(39, 1024)\n",
      "(38, 1024)\n",
      "(43, 1024)\n",
      "(39, 1024)\n",
      "(49, 1024)\n",
      "(49, 1024)\n",
      "(40, 1024)\n",
      "(107,) (35,) (107,) (35,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wave\n",
    "import librosa\n",
    "import re\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "# from bert_serving.client import BertClient\n",
    "\n",
    "prefix = '/Users/apple/Downloads/depression/'\n",
    "\n",
    "elmo = ElmoEmbedder()\n",
    "# bc = BertClient(ip='100.66.165.12')\n",
    "\n",
    "train_split_df = pd.read_csv(prefix+'train_split_Depression_AVEC2017 (1).csv')\n",
    "test_split_df = pd.read_csv(prefix+'dev_split_Depression_AVEC2017.csv')\n",
    "train_split_num = train_split_df[['Participant_ID']]['Participant_ID'].tolist()\n",
    "test_split_num = test_split_df[['Participant_ID']]['Participant_ID'].tolist()\n",
    "# train_split_clabel = train_split_df[['PHQ8_Binary']]['PHQ8_Binary'].tolist()\n",
    "# test_split_clabel = test_split_df[['PHQ8_Binary']]['PHQ8_Binary'].tolist()\n",
    "train_split_rlabel = train_split_df[['PHQ8_Score']]['PHQ8_Score'].tolist()\n",
    "test_split_rlabel = test_split_df[['PHQ8_Score']]['PHQ8_Score'].tolist()\n",
    "\n",
    "topics = []\n",
    "with open('/Users/apple/Downloads/depression/queries.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        topics.append(line.strip('\\n').strip())\n",
    "        \n",
    "\n",
    "def identify_topics(sentence):\n",
    "    if sentence in topics:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_features(number, text_features, target, mode, text_targets):\n",
    "    \n",
    "    transcript = pd.read_csv(prefix+'{0}_P/{0}_TRANSCRIPT.csv'.format(number), sep='\\t').fillna('')\n",
    "    \n",
    "    \n",
    "    time_range = []\n",
    "    responses = []\n",
    "    response = ''\n",
    "    response_flag = False\n",
    "    start_time = 0\n",
    "    stop_time = 0\n",
    "\n",
    "    signal = []\n",
    "    \n",
    "    global counter1, counter2\n",
    "\n",
    "    for t in transcript.itertuples():\n",
    "        # participant一句话结束\n",
    "        if getattr(t,'speaker') == 'Ellie':\n",
    "#             if '(' in getattr(t,'value'):\n",
    "#                 content = re.findall(re.compile(r'[(](.*?)[)]', re.S), getattr(t,'value'))[0]\n",
    "#                 print(content)\n",
    "#             else:\n",
    "#                 content = getattr(t,'value').strip()\n",
    "            content = getattr(t,'value').strip()\n",
    "            if identify_topics(content):\n",
    "                response_flag = True\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response.strip())\n",
    "                response = ''\n",
    "            elif response_flag and len(content.split()) > 4:\n",
    "                response_flag = False\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response)\n",
    "                response = ''\n",
    "        elif getattr(t,'speaker') == 'Participant':\n",
    "            if 'scrubbed_entry' in getattr(t,'value'):\n",
    "                continue\n",
    "            elif response_flag:\n",
    "                content = getattr(t,'value').split('\\n')[0].strip()\n",
    "#                 if '<' in getattr(t,'value'):\n",
    "#                     content = re.sub(u\"\\\\<.*?\\\\>\", \"\", content)\n",
    "                response+=' '+content\n",
    "        \n",
    "    text_feature = elmo.embed_sentence(responses).mean(0)\n",
    "#     text_feature = bc.encode(responses)\n",
    "#     while text_feature.shape[0] < 30:\n",
    "#         print(number)\n",
    "#         text_feature = np.vstack((text_feature, np.zeros(text_feature.shape[1])))\n",
    "    print(text_feature.shape)\n",
    "#     text_features.append(text_feature[:30])\n",
    "    text_features.append(text_feature)\n",
    "    text_targets.append(target)\n",
    "    \n",
    "def extract_features1(number, text_features, target, mode, text_targets):\n",
    "    \n",
    "    transcript = pd.read_csv(prefix+'{0}_P/{0}_TRANSCRIPT.csv'.format(number), sep='\\t').fillna('')\n",
    "    \n",
    "    \n",
    "    time_range = []\n",
    "    responses = []\n",
    "    response = ''\n",
    "    response_flag = False\n",
    "    start_time = 0\n",
    "    stop_time = 0\n",
    "\n",
    "    signal = []\n",
    "    \n",
    "    global counter1, counter2\n",
    "\n",
    "    for t in transcript.itertuples():\n",
    "        # participant一句话结束\n",
    "        if getattr(t,'speaker') == 'Ellie':\n",
    "            if '(' in getattr(t,'value'):\n",
    "                content = re.findall(re.compile(r'[(](.*?)[)]', re.S), getattr(t,'value'))[0]\n",
    "            else:\n",
    "                content = getattr(t,'value').strip()\n",
    "            content = getattr(t,'value').strip()\n",
    "            if identify_topics(content):\n",
    "                response_flag = True\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response.strip())\n",
    "                response = ''\n",
    "            elif response_flag and len(content.split()) > 4:\n",
    "                response_flag = False\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response)\n",
    "                response = ''\n",
    "        elif getattr(t,'speaker') == 'Participant':\n",
    "            if 'scrubbed_entry' in getattr(t,'value'):\n",
    "                continue\n",
    "            elif response_flag:\n",
    "                response+=' '+getattr(t,'value').split('\\n')[0].strip()\n",
    "        \n",
    "    text_feature = elmo.embed_sentence(responses).mean(0)\n",
    "    print(text_feature.shape)\n",
    "    text_features.append(text_feature)\n",
    "    if target == 1:\n",
    "        counter1 += len(text_feature)\n",
    "    else:\n",
    "        counter2 += len(text_feature)\n",
    "    text_targets.append(target)\n",
    "\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "    \n",
    "# training set\n",
    "text_features_train = []\n",
    "text_ctargets_train = []\n",
    "\n",
    "# test set\n",
    "text_features_test = []\n",
    "text_ctargets_test = []\n",
    "\n",
    "# ======================= classification =======================\n",
    "\n",
    "# training set\n",
    "# for index in range(len(train_split_num)):\n",
    "#     extract_features(train_split_num[index], text_features_train, train_split_clabel[index], 'train', text_ctargets_train)\n",
    "    \n",
    "# # test set\n",
    "# for index in range(len(test_split_num)):\n",
    "#     extract_features(test_split_num[index], text_features_test, test_split_clabel[index], 'test', text_ctargets_test)\n",
    "\n",
    "# ======================= classification =======================\n",
    "\n",
    "# training set\n",
    "for index in range(len(train_split_num)):\n",
    "    extract_features(train_split_num[index], text_features_train, train_split_rlabel[index], 'train', text_ctargets_train)\n",
    "    \n",
    "# test set\n",
    "for index in range(len(test_split_num)):\n",
    "    extract_features(test_split_num[index], text_features_test, test_split_rlabel[index], 'test', text_ctargets_test)\n",
    "print(np.shape(text_features_train), np.shape(text_features_test), np.shape(text_ctargets_train), np.shape(text_ctargets_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Saving npz file locally...\")\n",
    "\n",
    "# np.savez(prefix+'data/text/train_samples.npz', text_features_train)\n",
    "# np.savez(prefix+'data/text/train_labels.npz', text_features_test)\n",
    "# np.savez(prefix+'data/text/test_samples.npz', text_ctargets_train)\n",
    "# np.savez(prefix+'data/text/test_labels.npz', text_ctargets_test)\n",
    "\n",
    "prefix = '/Users/apple/Downloads/depression/'\n",
    "\n",
    "text_features_train = np.load(prefix+'data/text/train_samples.npz')['arr_0']\n",
    "text_features_test = np.load(prefix+'data/text/train_labels.npz')['arr_0']\n",
    "text_ctargets_train = np.load(prefix+'data/text/test_samples.npz')['arr_0']\n",
    "text_ctargets_test = np.load(prefix+'data/text/test_labels.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # 双层lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=self.bidirectional)\n",
    "        \n",
    "#         self.init_weight()\n",
    "        \n",
    "        # FC层\n",
    "#         self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "#         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imbalance\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "cut = 10\n",
    "debt = 0\n",
    "\n",
    "for i in range(len(text_features_train)):\n",
    "#     if text_ctargets_train[i] == 1:\n",
    "    if text_ctargets_train[i] >= 10:\n",
    "        times = 3+debt if counter < 46 else 2+debt\n",
    "#         print(times, text_features_train[i].shape, debt)\n",
    "        for j in range(times):\n",
    "            if (j+1)*cut > len(text_features_train[i]):\n",
    "                debt+=1\n",
    "                continue\n",
    "            X_train.append(text_features_train[i][j*cut:(j+1)*cut])\n",
    "            Y_train.append(text_ctargets_train[i])\n",
    "            if debt > 0:\n",
    "                debt -= 1\n",
    "            counter+=1\n",
    "    else:\n",
    "        X_train.append(text_features_train[i][:cut])\n",
    "        Y_train.append(text_ctargets_train[i])\n",
    "        \n",
    "        \n",
    "for i in range(len(text_features_test)):\n",
    "    X_test.append(text_features_test[i][:cut])\n",
    "    Y_test.append(text_ctargets_test[i])\n",
    "\n",
    "# for i in range(len(text_features_train)):\n",
    "#     if text_ctargets_train[i] == 1:\n",
    "#         times = int(len(text_features_train[i]) / 10)\n",
    "#         for j in range(times):\n",
    "#             X_train.append(text_features_train[i][j*10:(j+1)*10])\n",
    "#             Y_train.append(text_ctargets_train[i])\n",
    "#             counter+=1\n",
    "#     else:\n",
    "#         times = \n",
    "#         X_train.append(text_features_train[i][:10])\n",
    "#         Y_train.append(text_ctargets_train[i])\n",
    "        \n",
    "        \n",
    "# for i in range(len(text_features_test)):\n",
    "#     X_test.append(text_features_test[i][:10])\n",
    "#     Y_test.append(text_ctargets_test[i])\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 10, 1024)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'embedding_size': 1024,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 200,\n",
    "    'learning_rate': 5e-4,\n",
    "    'hidden_dims': 128,\n",
    "    'bidirectional': True\n",
    "}\n",
    "\n",
    "model = BiLSTM(config)\n",
    "\n",
    "# if args.cuda:\n",
    "#     model = model.cuda()\n",
    "#     X_train = X_train.cuda()\n",
    "#     Y_train = Y_train.cuda()\n",
    "#     X_test = X_test.cuda()\n",
    "#     Y_test = Y_test.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "max_f1 = -1\n",
    "max_acc = -1\n",
    "train_acc = -1\n",
    "min_mae = 100\n",
    "\n",
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "    \n",
    "def standard_confusion_matrix(y_test, y_test_pred):\n",
    "    \"\"\"\n",
    "    Make confusion matrix with format:\n",
    "                  -----------\n",
    "                  | TP | FP |\n",
    "                  -----------\n",
    "                  | FN | TN |\n",
    "                  -----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray - 1D\n",
    "    y_pred : ndarray - 1D\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray - 2D\n",
    "    \"\"\"\n",
    "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
    "    return np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "def model_performance(y_test, y_test_pred_proba):\n",
    "    \"\"\"\n",
    "    Evaluation metrics for network performance.\n",
    "    \"\"\"\n",
    "    y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
    "\n",
    "    # Computing confusion matrix for test dataset\n",
    "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return y_test_pred, conf_matrix\n",
    "\n",
    "def plot_roc_curve(y_test, y_score):\n",
    "    \"\"\"\n",
    "    Plots ROC curve for final trained model. Code taken from:\n",
    "    https://vkolachalama.blogspot.com/2016/05/keras-implementation-of-mlp-neural.html\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(prefix+'images/BiLSTM_roc.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global lr, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for i in range(0, X_train.shape[0], config['batch_size']):\n",
    "        if i + config['batch_size'] > X_train.shape[0]:\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
    "        if False:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        else:\n",
    "#             x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), Variable(torch.from_numpy(y))\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True), Variable(torch.from_numpy(y)).type(torch.FloatTensor)\n",
    "        # 将模型的参数梯度设置为0\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "#         pred = output.data.max(1, keepdim=True)[1]\n",
    "#         correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        loss = criterion(output.flatten(), y)\n",
    "        # 后向传播调整参数\n",
    "        loss.backward()\n",
    "        # 根据梯度更新网络参数\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        # loss.item()能够得到张量中的元素值\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "#     train_acc = correct\n",
    "    train_acc = total_loss/batch_idx\n",
    "    cur_loss = total_loss\n",
    "#     print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)\\n '.format(\n",
    "#                 epoch+1, config['learning_rate'], cur_loss, correct, len(X_train),\n",
    "#         100. * correct / len(X_train)))\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.6f}\\t '.format(\n",
    "                epoch+1, config['learning_rate'], total_loss/batch_idx))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    global max_f1, max_acc, min_mae\n",
    "    with torch.no_grad():\n",
    "        x, y = Variable(torch.from_numpy(X_test).type(torch.FloatTensor), requires_grad=True), Variable(torch.from_numpy(Y_test)).type(torch.FloatTensor)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output.flatten(), y)\n",
    "        total_loss += loss.item()\n",
    "#         print(y, output)\n",
    "#         y_test_pred, conf_matrix = model_performance(y, output)\n",
    "        print('\\nTest set: Average loss: {:.4f} \\t MAE: {:.4f}\\n'.format(total_loss, F.l1_loss(output.flatten(), y)))\n",
    "        \n",
    "#         # custom evaluation metrics\n",
    "#         print('Calculating additional test metrics...')\n",
    "#         accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "#         precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
    "#         recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
    "#         f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#         print(\"Accuracy: {}\".format(accuracy))\n",
    "#         print(\"Precision: {}\".format(precision))\n",
    "#         print(\"Recall: {}\".format(recall))\n",
    "#         print(\"F1-Score: {}\\n\".format(f1_score))\n",
    "#         print('='*89)\n",
    "        \n",
    "#         if max_f1 <= f1_score and train_acc > 151:\n",
    "#             max_f1 = f1_score\n",
    "#             max_acc = accuracy\n",
    "#             save(model, 'BiLSTM_elmo_{}_{:.2f}'.format(config['hidden_dims'], max_f1))          \n",
    "#             print('*'*64)\n",
    "#             print('model saved: f1: {}\\tacc: {}'.format(max_f1, max_acc))\n",
    "#             print('*'*64)\n",
    "        if min_mae >= F.l1_loss(output.flatten(), y) and train_acc < 2.0:\n",
    "            min_mae = F.l1_loss(output.flatten(), y)\n",
    "            save(model, 'BiLSTM_reg_{}_{:.2f}'.format(config['hidden_dims'], min_mae))          \n",
    "            print('*'*64)\n",
    "            print('model saved: f1: {}\\tacc: {}'.format(min_mae, train_acc))\n",
    "            print('*'*64)\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  2\t Learning rate: 0.0005\t Loss: 6.222528\t \n",
      "\n",
      "Test set: Average loss: 5.2454 \t MAE: 5.7352\n",
      "\n",
      "Train Epoch:  3\t Learning rate: 0.0005\t Loss: 4.801173\t \n",
      "\n",
      "Test set: Average loss: 5.5409 \t MAE: 6.0273\n",
      "\n",
      "Train Epoch:  4\t Learning rate: 0.0005\t Loss: 4.431237\t \n",
      "\n",
      "Test set: Average loss: 5.2539 \t MAE: 5.7438\n",
      "\n",
      "Train Epoch:  5\t Learning rate: 0.0005\t Loss: 4.382258\t \n",
      "\n",
      "Test set: Average loss: 5.3497 \t MAE: 5.8280\n",
      "\n",
      "Train Epoch:  6\t Learning rate: 0.0005\t Loss: 4.518691\t \n",
      "\n",
      "Test set: Average loss: 5.4297 \t MAE: 5.9101\n",
      "\n",
      "Train Epoch:  7\t Learning rate: 0.0005\t Loss: 4.425840\t \n",
      "\n",
      "Test set: Average loss: 5.3102 \t MAE: 5.7929\n",
      "\n",
      "Train Epoch:  8\t Learning rate: 0.0005\t Loss: 4.562018\t \n",
      "\n",
      "Test set: Average loss: 5.0805 \t MAE: 5.5750\n",
      "\n",
      "Train Epoch:  9\t Learning rate: 0.0005\t Loss: 4.654327\t \n",
      "\n",
      "Test set: Average loss: 5.2846 \t MAE: 5.7715\n",
      "\n",
      "Train Epoch: 10\t Learning rate: 0.0005\t Loss: 4.565650\t \n",
      "\n",
      "Test set: Average loss: 5.2604 \t MAE: 5.7470\n",
      "\n",
      "Train Epoch: 11\t Learning rate: 0.0005\t Loss: 4.451685\t \n",
      "\n",
      "Test set: Average loss: 5.2530 \t MAE: 5.7409\n",
      "\n",
      "Train Epoch: 12\t Learning rate: 0.0005\t Loss: 4.512582\t \n",
      "\n",
      "Test set: Average loss: 5.1506 \t MAE: 5.6465\n",
      "\n",
      "Train Epoch: 13\t Learning rate: 0.0005\t Loss: 4.443742\t \n",
      "\n",
      "Test set: Average loss: 5.1558 \t MAE: 5.6496\n",
      "\n",
      "Train Epoch: 14\t Learning rate: 0.0005\t Loss: 4.298081\t \n",
      "\n",
      "Test set: Average loss: 4.8663 \t MAE: 5.3347\n",
      "\n",
      "Train Epoch: 15\t Learning rate: 0.0005\t Loss: 3.788248\t \n",
      "\n",
      "Test set: Average loss: 4.2885 \t MAE: 4.7878\n",
      "\n",
      "Train Epoch: 16\t Learning rate: 0.0005\t Loss: 3.246390\t \n",
      "\n",
      "Test set: Average loss: 4.1861 \t MAE: 4.6585\n",
      "\n",
      "Train Epoch: 17\t Learning rate: 0.0005\t Loss: 4.021637\t \n",
      "\n",
      "Test set: Average loss: 3.8039 \t MAE: 4.2900\n",
      "\n",
      "Train Epoch: 18\t Learning rate: 0.0005\t Loss: 3.298809\t \n",
      "\n",
      "Test set: Average loss: 3.8797 \t MAE: 4.3641\n",
      "\n",
      "Train Epoch: 19\t Learning rate: 0.0005\t Loss: 2.909718\t \n",
      "\n",
      "Test set: Average loss: 3.6439 \t MAE: 4.1346\n",
      "\n",
      "Train Epoch: 20\t Learning rate: 0.0005\t Loss: 2.991214\t \n",
      "\n",
      "Test set: Average loss: 3.5464 \t MAE: 4.0234\n",
      "\n",
      "Train Epoch: 21\t Learning rate: 0.0005\t Loss: 2.892707\t \n",
      "\n",
      "Test set: Average loss: 3.1266 \t MAE: 3.5861\n",
      "\n",
      "Train Epoch: 22\t Learning rate: 0.0005\t Loss: 2.806894\t \n",
      "\n",
      "Test set: Average loss: 3.3687 \t MAE: 3.8398\n",
      "\n",
      "Train Epoch: 23\t Learning rate: 0.0005\t Loss: 2.462016\t \n",
      "\n",
      "Test set: Average loss: 3.2983 \t MAE: 3.7604\n",
      "\n",
      "Train Epoch: 24\t Learning rate: 0.0005\t Loss: 2.452944\t \n",
      "\n",
      "Test set: Average loss: 3.7188 \t MAE: 4.2055\n",
      "\n",
      "Train Epoch: 25\t Learning rate: 0.0005\t Loss: 2.329042\t \n",
      "\n",
      "Test set: Average loss: 3.4484 \t MAE: 3.8869\n",
      "\n",
      "Train Epoch: 26\t Learning rate: 0.0005\t Loss: 2.495918\t \n",
      "\n",
      "Test set: Average loss: 3.8797 \t MAE: 4.3500\n",
      "\n",
      "Train Epoch: 27\t Learning rate: 0.0005\t Loss: 2.272594\t \n",
      "\n",
      "Test set: Average loss: 3.8090 \t MAE: 4.2980\n",
      "\n",
      "Train Epoch: 28\t Learning rate: 0.0005\t Loss: 2.619707\t \n",
      "\n",
      "Test set: Average loss: 3.6732 \t MAE: 4.1515\n",
      "\n",
      "Train Epoch: 29\t Learning rate: 0.0005\t Loss: 2.312229\t \n",
      "\n",
      "Test set: Average loss: 3.6419 \t MAE: 4.1090\n",
      "\n",
      "Train Epoch: 30\t Learning rate: 0.0005\t Loss: 2.056890\t \n",
      "\n",
      "Test set: Average loss: 4.1940 \t MAE: 4.6646\n",
      "\n",
      "Train Epoch: 31\t Learning rate: 0.0005\t Loss: 2.042540\t \n",
      "\n",
      "Test set: Average loss: 3.8574 \t MAE: 4.3044\n",
      "\n",
      "Train Epoch: 32\t Learning rate: 0.0005\t Loss: 2.110537\t \n",
      "\n",
      "Test set: Average loss: 3.8590 \t MAE: 4.3267\n",
      "\n",
      "Train Epoch: 33\t Learning rate: 0.0005\t Loss: 2.181078\t \n",
      "\n",
      "Test set: Average loss: 4.0800 \t MAE: 4.5380\n",
      "\n",
      "Train Epoch: 34\t Learning rate: 0.0005\t Loss: 1.838920\t \n",
      "\n",
      "Test set: Average loss: 3.6816 \t MAE: 4.1475\n",
      "\n",
      "Saved as BiLSTM_reg_128_4.15.pt\n",
      "****************************************************************\n",
      "model saved: f1: 4.147465705871582\tacc: 1.8389204747620083\n",
      "****************************************************************\n",
      "Train Epoch: 35\t Learning rate: 0.0005\t Loss: 1.695412\t \n",
      "\n",
      "Test set: Average loss: 4.4402 \t MAE: 4.9363\n",
      "\n",
      "Train Epoch: 36\t Learning rate: 0.0005\t Loss: 2.076556\t \n",
      "\n",
      "Test set: Average loss: 4.0873 \t MAE: 4.5635\n",
      "\n",
      "Train Epoch: 37\t Learning rate: 0.0005\t Loss: 1.879723\t \n",
      "\n",
      "Test set: Average loss: 4.5997 \t MAE: 5.0551\n",
      "\n",
      "Train Epoch: 38\t Learning rate: 0.0005\t Loss: 1.973436\t \n",
      "\n",
      "Test set: Average loss: 4.1427 \t MAE: 4.6181\n",
      "\n",
      "Train Epoch: 39\t Learning rate: 0.0005\t Loss: 1.913318\t \n",
      "\n",
      "Test set: Average loss: 4.1102 \t MAE: 4.5848\n",
      "\n",
      "Train Epoch: 40\t Learning rate: 0.0005\t Loss: 1.856847\t \n",
      "\n",
      "Test set: Average loss: 4.1139 \t MAE: 4.5657\n",
      "\n",
      "Train Epoch: 41\t Learning rate: 0.0005\t Loss: 1.771701\t \n",
      "\n",
      "Test set: Average loss: 3.9007 \t MAE: 4.3717\n",
      "\n",
      "Train Epoch: 42\t Learning rate: 0.0005\t Loss: 1.764812\t \n",
      "\n",
      "Test set: Average loss: 3.7831 \t MAE: 4.2577\n",
      "\n",
      "Train Epoch: 43\t Learning rate: 0.0005\t Loss: 1.636393\t \n",
      "\n",
      "Test set: Average loss: 3.9092 \t MAE: 4.3770\n",
      "\n",
      "Train Epoch: 44\t Learning rate: 0.0005\t Loss: 1.874798\t \n",
      "\n",
      "Test set: Average loss: 4.0795 \t MAE: 4.5496\n",
      "\n",
      "Train Epoch: 45\t Learning rate: 0.0005\t Loss: 1.580698\t \n",
      "\n",
      "Test set: Average loss: 4.1065 \t MAE: 4.5840\n",
      "\n",
      "Train Epoch: 46\t Learning rate: 0.0005\t Loss: 1.548155\t \n",
      "\n",
      "Test set: Average loss: 4.0280 \t MAE: 4.5011\n",
      "\n",
      "Train Epoch: 47\t Learning rate: 0.0005\t Loss: 1.400702\t \n",
      "\n",
      "Test set: Average loss: 4.2299 \t MAE: 4.6754\n",
      "\n",
      "Train Epoch: 48\t Learning rate: 0.0005\t Loss: 1.549356\t \n",
      "\n",
      "Test set: Average loss: 4.5074 \t MAE: 4.9787\n",
      "\n",
      "Train Epoch: 49\t Learning rate: 0.0005\t Loss: 1.574735\t \n",
      "\n",
      "Test set: Average loss: 4.5332 \t MAE: 4.9923\n",
      "\n",
      "Train Epoch: 50\t Learning rate: 0.0005\t Loss: 1.354524\t \n",
      "\n",
      "Test set: Average loss: 4.7064 \t MAE: 5.1539\n",
      "\n",
      "Train Epoch: 51\t Learning rate: 0.0005\t Loss: 2.049156\t \n",
      "\n",
      "Test set: Average loss: 4.1546 \t MAE: 4.6241\n",
      "\n",
      "Train Epoch: 52\t Learning rate: 0.0005\t Loss: 1.527975\t \n",
      "\n",
      "Test set: Average loss: 4.0918 \t MAE: 4.5543\n",
      "\n",
      "Train Epoch: 53\t Learning rate: 0.0005\t Loss: 1.346940\t \n",
      "\n",
      "Test set: Average loss: 4.0010 \t MAE: 4.4820\n",
      "\n",
      "Train Epoch: 54\t Learning rate: 0.0005\t Loss: 1.353701\t \n",
      "\n",
      "Test set: Average loss: 4.3962 \t MAE: 4.8609\n",
      "\n",
      "Train Epoch: 55\t Learning rate: 0.0005\t Loss: 1.306012\t \n",
      "\n",
      "Test set: Average loss: 4.2128 \t MAE: 4.6894\n",
      "\n",
      "Train Epoch: 56\t Learning rate: 0.0005\t Loss: 1.371004\t \n",
      "\n",
      "Test set: Average loss: 4.1537 \t MAE: 4.6346\n",
      "\n",
      "Train Epoch: 57\t Learning rate: 0.0005\t Loss: 1.368044\t \n",
      "\n",
      "Test set: Average loss: 4.2551 \t MAE: 4.7531\n",
      "\n",
      "Train Epoch: 58\t Learning rate: 0.0005\t Loss: 1.447491\t \n",
      "\n",
      "Test set: Average loss: 4.0097 \t MAE: 4.4771\n",
      "\n",
      "Train Epoch: 59\t Learning rate: 0.0005\t Loss: 1.247104\t \n",
      "\n",
      "Test set: Average loss: 3.9311 \t MAE: 4.4053\n",
      "\n",
      "Train Epoch: 60\t Learning rate: 0.0005\t Loss: 1.315407\t \n",
      "\n",
      "Test set: Average loss: 4.3192 \t MAE: 4.7980\n",
      "\n",
      "Train Epoch: 61\t Learning rate: 0.0005\t Loss: 1.247827\t \n",
      "\n",
      "Test set: Average loss: 4.2105 \t MAE: 4.6818\n",
      "\n",
      "Train Epoch: 62\t Learning rate: 0.0005\t Loss: 1.415605\t \n",
      "\n",
      "Test set: Average loss: 4.3634 \t MAE: 4.8293\n",
      "\n",
      "Train Epoch: 63\t Learning rate: 0.0005\t Loss: 1.222708\t \n",
      "\n",
      "Test set: Average loss: 4.5952 \t MAE: 5.0836\n",
      "\n",
      "Train Epoch: 64\t Learning rate: 0.0005\t Loss: 1.311415\t \n",
      "\n",
      "Test set: Average loss: 4.2124 \t MAE: 4.6954\n",
      "\n",
      "Train Epoch: 65\t Learning rate: 0.0005\t Loss: 1.487544\t \n",
      "\n",
      "Test set: Average loss: 4.0394 \t MAE: 4.4944\n",
      "\n",
      "Train Epoch: 66\t Learning rate: 0.0005\t Loss: 1.071867\t \n",
      "\n",
      "Test set: Average loss: 3.9212 \t MAE: 4.3847\n",
      "\n",
      "Train Epoch: 67\t Learning rate: 0.0005\t Loss: 1.151054\t \n",
      "\n",
      "Test set: Average loss: 4.4762 \t MAE: 4.9519\n",
      "\n",
      "Train Epoch: 68\t Learning rate: 0.0005\t Loss: 1.309198\t \n",
      "\n",
      "Test set: Average loss: 4.4172 \t MAE: 4.8880\n",
      "\n",
      "Train Epoch: 69\t Learning rate: 0.0005\t Loss: 1.121209\t \n",
      "\n",
      "Test set: Average loss: 4.4038 \t MAE: 4.8850\n",
      "\n",
      "Train Epoch: 70\t Learning rate: 0.0005\t Loss: 1.122766\t \n",
      "\n",
      "Test set: Average loss: 3.9661 \t MAE: 4.4481\n",
      "\n",
      "Train Epoch: 71\t Learning rate: 0.0005\t Loss: 1.306642\t \n",
      "\n",
      "Test set: Average loss: 4.0182 \t MAE: 4.4837\n",
      "\n",
      "Train Epoch: 72\t Learning rate: 0.0005\t Loss: 1.271639\t \n",
      "\n",
      "Test set: Average loss: 4.3392 \t MAE: 4.7997\n",
      "\n",
      "Train Epoch: 73\t Learning rate: 0.0005\t Loss: 1.111040\t \n",
      "\n",
      "Test set: Average loss: 4.1592 \t MAE: 4.6452\n",
      "\n",
      "Train Epoch: 74\t Learning rate: 0.0005\t Loss: 1.092647\t \n",
      "\n",
      "Test set: Average loss: 4.5354 \t MAE: 5.0171\n",
      "\n",
      "Train Epoch: 75\t Learning rate: 0.0005\t Loss: 1.071747\t \n",
      "\n",
      "Test set: Average loss: 4.2062 \t MAE: 4.6748\n",
      "\n",
      "Train Epoch: 76\t Learning rate: 0.0005\t Loss: 1.139450\t \n",
      "\n",
      "Test set: Average loss: 3.7759 \t MAE: 4.2505\n",
      "\n",
      "Train Epoch: 77\t Learning rate: 0.0005\t Loss: 1.193882\t \n",
      "\n",
      "Test set: Average loss: 3.6220 \t MAE: 4.0822\n",
      "\n",
      "Saved as BiLSTM_reg_128_4.08.pt\n",
      "****************************************************************\n",
      "model saved: f1: 4.082216739654541\tacc: 1.1938817671367101\n",
      "****************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 78\t Learning rate: 0.0005\t Loss: 1.062055\t \n",
      "\n",
      "Test set: Average loss: 3.9293 \t MAE: 4.4046\n",
      "\n",
      "Train Epoch: 79\t Learning rate: 0.0005\t Loss: 1.161488\t \n",
      "\n",
      "Test set: Average loss: 4.0311 \t MAE: 4.4819\n",
      "\n",
      "Train Epoch: 80\t Learning rate: 0.0005\t Loss: 1.155397\t \n",
      "\n",
      "Test set: Average loss: 4.4762 \t MAE: 4.9510\n",
      "\n",
      "Train Epoch: 81\t Learning rate: 0.0005\t Loss: 1.124911\t \n",
      "\n",
      "Test set: Average loss: 3.9555 \t MAE: 4.4218\n",
      "\n",
      "Train Epoch: 82\t Learning rate: 0.0005\t Loss: 1.087112\t \n",
      "\n",
      "Test set: Average loss: 3.9758 \t MAE: 4.4648\n",
      "\n",
      "Train Epoch: 83\t Learning rate: 0.0005\t Loss: 1.143451\t \n",
      "\n",
      "Test set: Average loss: 4.1492 \t MAE: 4.6386\n",
      "\n",
      "Train Epoch: 84\t Learning rate: 0.0005\t Loss: 1.021506\t \n",
      "\n",
      "Test set: Average loss: 4.2033 \t MAE: 4.6706\n",
      "\n",
      "Train Epoch: 85\t Learning rate: 0.0005\t Loss: 1.397525\t \n",
      "\n",
      "Test set: Average loss: 4.3981 \t MAE: 4.8657\n",
      "\n",
      "Train Epoch: 86\t Learning rate: 0.0005\t Loss: 0.968711\t \n",
      "\n",
      "Test set: Average loss: 3.7188 \t MAE: 4.2074\n",
      "\n",
      "Train Epoch: 87\t Learning rate: 0.0005\t Loss: 1.095073\t \n",
      "\n",
      "Test set: Average loss: 3.9837 \t MAE: 4.4526\n",
      "\n",
      "Train Epoch: 88\t Learning rate: 0.0005\t Loss: 1.061223\t \n",
      "\n",
      "Test set: Average loss: 4.3055 \t MAE: 4.7557\n",
      "\n",
      "Train Epoch: 89\t Learning rate: 0.0005\t Loss: 0.967244\t \n",
      "\n",
      "Test set: Average loss: 3.9507 \t MAE: 4.4074\n",
      "\n",
      "Train Epoch: 90\t Learning rate: 0.0005\t Loss: 1.116576\t \n",
      "\n",
      "Test set: Average loss: 4.2355 \t MAE: 4.7020\n",
      "\n",
      "Train Epoch: 91\t Learning rate: 0.0005\t Loss: 1.129589\t \n",
      "\n",
      "Test set: Average loss: 4.2573 \t MAE: 4.7175\n",
      "\n",
      "Train Epoch: 92\t Learning rate: 0.0005\t Loss: 0.998689\t \n",
      "\n",
      "Test set: Average loss: 4.2436 \t MAE: 4.7121\n",
      "\n",
      "Train Epoch: 93\t Learning rate: 0.0005\t Loss: 1.028936\t \n",
      "\n",
      "Test set: Average loss: 4.2015 \t MAE: 4.6581\n",
      "\n",
      "Train Epoch: 94\t Learning rate: 0.0005\t Loss: 1.025468\t \n",
      "\n",
      "Test set: Average loss: 4.2462 \t MAE: 4.6956\n",
      "\n",
      "Train Epoch: 95\t Learning rate: 0.0005\t Loss: 0.973014\t \n",
      "\n",
      "Test set: Average loss: 3.9069 \t MAE: 4.3618\n",
      "\n",
      "Train Epoch: 96\t Learning rate: 0.0005\t Loss: 0.917344\t \n",
      "\n",
      "Test set: Average loss: 4.2787 \t MAE: 4.7302\n",
      "\n",
      "Train Epoch: 97\t Learning rate: 0.0005\t Loss: 1.120929\t \n",
      "\n",
      "Test set: Average loss: 4.3165 \t MAE: 4.7986\n",
      "\n",
      "Train Epoch: 98\t Learning rate: 0.0005\t Loss: 0.962194\t \n",
      "\n",
      "Test set: Average loss: 3.8568 \t MAE: 4.3219\n",
      "\n",
      "Train Epoch: 99\t Learning rate: 0.0005\t Loss: 1.249937\t \n",
      "\n",
      "Test set: Average loss: 4.0346 \t MAE: 4.4936\n",
      "\n",
      "Train Epoch: 100\t Learning rate: 0.0005\t Loss: 1.070985\t \n",
      "\n",
      "Test set: Average loss: 4.2920 \t MAE: 4.7649\n",
      "\n",
      "Train Epoch: 101\t Learning rate: 0.0005\t Loss: 0.921444\t \n",
      "\n",
      "Test set: Average loss: 4.0885 \t MAE: 4.5694\n",
      "\n",
      "Train Epoch: 102\t Learning rate: 0.0005\t Loss: 1.260023\t \n",
      "\n",
      "Test set: Average loss: 4.1256 \t MAE: 4.5917\n",
      "\n",
      "Train Epoch: 103\t Learning rate: 0.0005\t Loss: 1.159951\t \n",
      "\n",
      "Test set: Average loss: 4.2488 \t MAE: 4.7348\n",
      "\n",
      "Train Epoch: 104\t Learning rate: 0.0005\t Loss: 1.322786\t \n",
      "\n",
      "Test set: Average loss: 4.3939 \t MAE: 4.8783\n",
      "\n",
      "Train Epoch: 105\t Learning rate: 0.0005\t Loss: 0.962339\t \n",
      "\n",
      "Test set: Average loss: 4.4167 \t MAE: 4.8781\n",
      "\n",
      "Train Epoch: 106\t Learning rate: 0.0005\t Loss: 0.944769\t \n",
      "\n",
      "Test set: Average loss: 4.1260 \t MAE: 4.6052\n",
      "\n",
      "Train Epoch: 107\t Learning rate: 0.0005\t Loss: 1.055879\t \n",
      "\n",
      "Test set: Average loss: 4.4353 \t MAE: 4.8979\n",
      "\n",
      "Train Epoch: 108\t Learning rate: 0.0005\t Loss: 0.929050\t \n",
      "\n",
      "Test set: Average loss: 4.3397 \t MAE: 4.7854\n",
      "\n",
      "Train Epoch: 109\t Learning rate: 0.0005\t Loss: 0.961483\t \n",
      "\n",
      "Test set: Average loss: 3.9624 \t MAE: 4.4224\n",
      "\n",
      "Train Epoch: 110\t Learning rate: 0.0005\t Loss: 0.946940\t \n",
      "\n",
      "Test set: Average loss: 4.2200 \t MAE: 4.6670\n",
      "\n",
      "Train Epoch: 111\t Learning rate: 0.0005\t Loss: 0.918805\t \n",
      "\n",
      "Test set: Average loss: 3.9667 \t MAE: 4.4206\n",
      "\n",
      "Train Epoch: 112\t Learning rate: 0.0005\t Loss: 1.147821\t \n",
      "\n",
      "Test set: Average loss: 4.0068 \t MAE: 4.4806\n",
      "\n",
      "Train Epoch: 113\t Learning rate: 0.0005\t Loss: 1.246980\t \n",
      "\n",
      "Test set: Average loss: 4.1982 \t MAE: 4.6641\n",
      "\n",
      "Train Epoch: 114\t Learning rate: 0.0005\t Loss: 1.113731\t \n",
      "\n",
      "Test set: Average loss: 4.4716 \t MAE: 4.9515\n",
      "\n",
      "Train Epoch: 115\t Learning rate: 0.0005\t Loss: 0.942133\t \n",
      "\n",
      "Test set: Average loss: 3.8472 \t MAE: 4.3116\n",
      "\n",
      "Train Epoch: 116\t Learning rate: 0.0005\t Loss: 0.836111\t \n",
      "\n",
      "Test set: Average loss: 3.7778 \t MAE: 4.2443\n",
      "\n",
      "Train Epoch: 117\t Learning rate: 0.0005\t Loss: 1.042380\t \n",
      "\n",
      "Test set: Average loss: 3.8928 \t MAE: 4.3669\n",
      "\n",
      "Train Epoch: 118\t Learning rate: 0.0005\t Loss: 0.888997\t \n",
      "\n",
      "Test set: Average loss: 4.3176 \t MAE: 4.7722\n",
      "\n",
      "Train Epoch: 119\t Learning rate: 0.0005\t Loss: 0.863362\t \n",
      "\n",
      "Test set: Average loss: 3.7324 \t MAE: 4.2109\n",
      "\n",
      "Train Epoch: 120\t Learning rate: 0.0005\t Loss: 1.225582\t \n",
      "\n",
      "Test set: Average loss: 4.1852 \t MAE: 4.6686\n",
      "\n",
      "Train Epoch: 121\t Learning rate: 0.0005\t Loss: 1.197303\t \n",
      "\n",
      "Test set: Average loss: 4.4508 \t MAE: 4.9219\n",
      "\n",
      "Train Epoch: 122\t Learning rate: 0.0005\t Loss: 1.091303\t \n",
      "\n",
      "Test set: Average loss: 4.3728 \t MAE: 4.8488\n",
      "\n",
      "Train Epoch: 123\t Learning rate: 0.0005\t Loss: 1.260954\t \n",
      "\n",
      "Test set: Average loss: 4.1683 \t MAE: 4.6418\n",
      "\n",
      "Train Epoch: 124\t Learning rate: 0.0005\t Loss: 0.879488\t \n",
      "\n",
      "Test set: Average loss: 3.9878 \t MAE: 4.4624\n",
      "\n",
      "Train Epoch: 125\t Learning rate: 0.0005\t Loss: 1.115321\t \n",
      "\n",
      "Test set: Average loss: 3.9576 \t MAE: 4.4411\n",
      "\n",
      "Train Epoch: 126\t Learning rate: 0.0005\t Loss: 0.900170\t \n",
      "\n",
      "Test set: Average loss: 4.1885 \t MAE: 4.6625\n",
      "\n",
      "Train Epoch: 127\t Learning rate: 0.0005\t Loss: 0.914275\t \n",
      "\n",
      "Test set: Average loss: 4.1754 \t MAE: 4.6519\n",
      "\n",
      "Train Epoch: 128\t Learning rate: 0.0005\t Loss: 0.845138\t \n",
      "\n",
      "Test set: Average loss: 4.3401 \t MAE: 4.8130\n",
      "\n",
      "Train Epoch: 129\t Learning rate: 0.0005\t Loss: 1.078165\t \n",
      "\n",
      "Test set: Average loss: 4.2290 \t MAE: 4.7133\n",
      "\n",
      "Train Epoch: 130\t Learning rate: 0.0005\t Loss: 0.893762\t \n",
      "\n",
      "Test set: Average loss: 4.2804 \t MAE: 4.7473\n",
      "\n",
      "Train Epoch: 131\t Learning rate: 0.0005\t Loss: 0.898790\t \n",
      "\n",
      "Test set: Average loss: 4.3800 \t MAE: 4.8504\n",
      "\n",
      "Train Epoch: 132\t Learning rate: 0.0005\t Loss: 0.943924\t \n",
      "\n",
      "Test set: Average loss: 4.0918 \t MAE: 4.5740\n",
      "\n",
      "Train Epoch: 133\t Learning rate: 0.0005\t Loss: 0.892788\t \n",
      "\n",
      "Test set: Average loss: 4.2037 \t MAE: 4.6698\n",
      "\n",
      "Train Epoch: 134\t Learning rate: 0.0005\t Loss: 0.993899\t \n",
      "\n",
      "Test set: Average loss: 4.5271 \t MAE: 4.9923\n",
      "\n",
      "Train Epoch: 135\t Learning rate: 0.0005\t Loss: 1.000830\t \n",
      "\n",
      "Test set: Average loss: 3.9788 \t MAE: 4.4592\n",
      "\n",
      "Train Epoch: 136\t Learning rate: 0.0005\t Loss: 0.865986\t \n",
      "\n",
      "Test set: Average loss: 3.9782 \t MAE: 4.4541\n",
      "\n",
      "Train Epoch: 137\t Learning rate: 0.0005\t Loss: 0.778416\t \n",
      "\n",
      "Test set: Average loss: 4.0501 \t MAE: 4.5125\n",
      "\n",
      "Train Epoch: 138\t Learning rate: 0.0005\t Loss: 0.899644\t \n",
      "\n",
      "Test set: Average loss: 3.9759 \t MAE: 4.4407\n",
      "\n",
      "Train Epoch: 139\t Learning rate: 0.0005\t Loss: 0.858805\t \n",
      "\n",
      "Test set: Average loss: 4.1885 \t MAE: 4.6453\n",
      "\n",
      "Train Epoch: 140\t Learning rate: 0.0005\t Loss: 0.980515\t \n",
      "\n",
      "Test set: Average loss: 4.2229 \t MAE: 4.6874\n",
      "\n",
      "Train Epoch: 141\t Learning rate: 0.0005\t Loss: 0.990140\t \n",
      "\n",
      "Test set: Average loss: 4.2213 \t MAE: 4.6812\n",
      "\n",
      "Train Epoch: 142\t Learning rate: 0.0005\t Loss: 1.142093\t \n",
      "\n",
      "Test set: Average loss: 4.4396 \t MAE: 4.9052\n",
      "\n",
      "Train Epoch: 143\t Learning rate: 0.0005\t Loss: 0.918046\t \n",
      "\n",
      "Test set: Average loss: 4.5148 \t MAE: 4.9913\n",
      "\n",
      "Train Epoch: 144\t Learning rate: 0.0005\t Loss: 0.942595\t \n",
      "\n",
      "Test set: Average loss: 4.2595 \t MAE: 4.7389\n",
      "\n",
      "Train Epoch: 145\t Learning rate: 0.0005\t Loss: 0.875068\t \n",
      "\n",
      "Test set: Average loss: 3.9844 \t MAE: 4.4673\n",
      "\n",
      "Train Epoch: 146\t Learning rate: 0.0005\t Loss: 0.932485\t \n",
      "\n",
      "Test set: Average loss: 4.4675 \t MAE: 4.9310\n",
      "\n",
      "Train Epoch: 147\t Learning rate: 0.0005\t Loss: 1.047656\t \n",
      "\n",
      "Test set: Average loss: 4.2100 \t MAE: 4.6831\n",
      "\n",
      "Train Epoch: 148\t Learning rate: 0.0005\t Loss: 0.835819\t \n",
      "\n",
      "Test set: Average loss: 4.0317 \t MAE: 4.5118\n",
      "\n",
      "Train Epoch: 149\t Learning rate: 0.0005\t Loss: 0.980606\t \n",
      "\n",
      "Test set: Average loss: 4.3075 \t MAE: 4.7642\n",
      "\n",
      "Train Epoch: 150\t Learning rate: 0.0005\t Loss: 0.830669\t \n",
      "\n",
      "Test set: Average loss: 4.5313 \t MAE: 4.9935\n",
      "\n",
      "Train Epoch: 151\t Learning rate: 0.0005\t Loss: 0.893053\t \n",
      "\n",
      "Test set: Average loss: 4.4451 \t MAE: 4.9095\n",
      "\n",
      "Train Epoch: 152\t Learning rate: 0.0005\t Loss: 0.872342\t \n",
      "\n",
      "Test set: Average loss: 4.2925 \t MAE: 4.7543\n",
      "\n",
      "Train Epoch: 153\t Learning rate: 0.0005\t Loss: 0.983376\t \n",
      "\n",
      "Test set: Average loss: 4.1181 \t MAE: 4.5934\n",
      "\n",
      "Train Epoch: 154\t Learning rate: 0.0005\t Loss: 1.020965\t \n",
      "\n",
      "Test set: Average loss: 4.1233 \t MAE: 4.5878\n",
      "\n",
      "Train Epoch: 155\t Learning rate: 0.0005\t Loss: 0.784421\t \n",
      "\n",
      "Test set: Average loss: 4.4377 \t MAE: 4.9053\n",
      "\n",
      "Train Epoch: 156\t Learning rate: 0.0005\t Loss: 1.126911\t \n",
      "\n",
      "Test set: Average loss: 3.9217 \t MAE: 4.4043\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 157\t Learning rate: 0.0005\t Loss: 0.992670\t \n",
      "\n",
      "Test set: Average loss: 4.0878 \t MAE: 4.5467\n",
      "\n",
      "Train Epoch: 158\t Learning rate: 0.0005\t Loss: 0.865013\t \n",
      "\n",
      "Test set: Average loss: 4.3303 \t MAE: 4.7961\n",
      "\n",
      "Train Epoch: 159\t Learning rate: 0.0005\t Loss: 0.814098\t \n",
      "\n",
      "Test set: Average loss: 4.0651 \t MAE: 4.5495\n",
      "\n",
      "Train Epoch: 160\t Learning rate: 0.0005\t Loss: 0.843722\t \n",
      "\n",
      "Test set: Average loss: 3.9392 \t MAE: 4.4237\n",
      "\n",
      "Train Epoch: 161\t Learning rate: 0.0005\t Loss: 0.778094\t \n",
      "\n",
      "Test set: Average loss: 4.1093 \t MAE: 4.5677\n",
      "\n",
      "Train Epoch: 162\t Learning rate: 0.0005\t Loss: 0.760861\t \n",
      "\n",
      "Test set: Average loss: 4.0424 \t MAE: 4.4901\n",
      "\n",
      "Train Epoch: 163\t Learning rate: 0.0005\t Loss: 0.976117\t \n",
      "\n",
      "Test set: Average loss: 3.9120 \t MAE: 4.3788\n",
      "\n",
      "Train Epoch: 164\t Learning rate: 0.0005\t Loss: 0.747449\t \n",
      "\n",
      "Test set: Average loss: 4.1145 \t MAE: 4.5898\n",
      "\n",
      "Train Epoch: 165\t Learning rate: 0.0005\t Loss: 0.960359\t \n",
      "\n",
      "Test set: Average loss: 4.3442 \t MAE: 4.8045\n",
      "\n",
      "Train Epoch: 166\t Learning rate: 0.0005\t Loss: 0.837411\t \n",
      "\n",
      "Test set: Average loss: 4.5189 \t MAE: 4.9757\n",
      "\n",
      "Train Epoch: 167\t Learning rate: 0.0005\t Loss: 0.917638\t \n",
      "\n",
      "Test set: Average loss: 4.2090 \t MAE: 4.6905\n",
      "\n",
      "Train Epoch: 168\t Learning rate: 0.0005\t Loss: 0.850689\t \n",
      "\n",
      "Test set: Average loss: 4.0824 \t MAE: 4.5608\n",
      "\n",
      "Train Epoch: 169\t Learning rate: 0.0005\t Loss: 0.974172\t \n",
      "\n",
      "Test set: Average loss: 4.0506 \t MAE: 4.5225\n",
      "\n",
      "Train Epoch: 170\t Learning rate: 0.0005\t Loss: 0.863295\t \n",
      "\n",
      "Test set: Average loss: 4.3983 \t MAE: 4.8468\n",
      "\n",
      "Train Epoch: 171\t Learning rate: 0.0005\t Loss: 0.931016\t \n",
      "\n",
      "Test set: Average loss: 4.1384 \t MAE: 4.6132\n",
      "\n",
      "Train Epoch: 172\t Learning rate: 0.0005\t Loss: 0.794198\t \n",
      "\n",
      "Test set: Average loss: 4.3596 \t MAE: 4.8161\n",
      "\n",
      "Train Epoch: 173\t Learning rate: 0.0005\t Loss: 0.820409\t \n",
      "\n",
      "Test set: Average loss: 4.1466 \t MAE: 4.6183\n",
      "\n",
      "Train Epoch: 174\t Learning rate: 0.0005\t Loss: 0.967401\t \n",
      "\n",
      "Test set: Average loss: 4.2012 \t MAE: 4.6818\n",
      "\n",
      "Train Epoch: 175\t Learning rate: 0.0005\t Loss: 0.931624\t \n",
      "\n",
      "Test set: Average loss: 4.0032 \t MAE: 4.4773\n",
      "\n",
      "Train Epoch: 176\t Learning rate: 0.0005\t Loss: 0.914719\t \n",
      "\n",
      "Test set: Average loss: 4.2812 \t MAE: 4.7415\n",
      "\n",
      "Train Epoch: 177\t Learning rate: 0.0005\t Loss: 0.853825\t \n",
      "\n",
      "Test set: Average loss: 4.1724 \t MAE: 4.6384\n",
      "\n",
      "Train Epoch: 178\t Learning rate: 0.0005\t Loss: 0.782554\t \n",
      "\n",
      "Test set: Average loss: 4.1872 \t MAE: 4.6614\n",
      "\n",
      "Train Epoch: 179\t Learning rate: 0.0005\t Loss: 0.790341\t \n",
      "\n",
      "Test set: Average loss: 4.3984 \t MAE: 4.8560\n",
      "\n",
      "Train Epoch: 180\t Learning rate: 0.0005\t Loss: 0.691040\t \n",
      "\n",
      "Test set: Average loss: 4.4033 \t MAE: 4.8658\n",
      "\n",
      "Train Epoch: 181\t Learning rate: 0.0005\t Loss: 0.850953\t \n",
      "\n",
      "Test set: Average loss: 3.8660 \t MAE: 4.3404\n",
      "\n",
      "Train Epoch: 182\t Learning rate: 0.0005\t Loss: 0.896042\t \n",
      "\n",
      "Test set: Average loss: 4.2568 \t MAE: 4.7351\n",
      "\n",
      "Train Epoch: 183\t Learning rate: 0.0005\t Loss: 0.960200\t \n",
      "\n",
      "Test set: Average loss: 4.3396 \t MAE: 4.7906\n",
      "\n",
      "Train Epoch: 184\t Learning rate: 0.0005\t Loss: 0.959877\t \n",
      "\n",
      "Test set: Average loss: 3.8674 \t MAE: 4.3530\n",
      "\n",
      "Train Epoch: 185\t Learning rate: 0.0005\t Loss: 0.725641\t \n",
      "\n",
      "Test set: Average loss: 3.9436 \t MAE: 4.4092\n",
      "\n",
      "Train Epoch: 186\t Learning rate: 0.0005\t Loss: 0.767895\t \n",
      "\n",
      "Test set: Average loss: 4.2262 \t MAE: 4.6659\n",
      "\n",
      "Train Epoch: 187\t Learning rate: 0.0005\t Loss: 0.843481\t \n",
      "\n",
      "Test set: Average loss: 4.2961 \t MAE: 4.7634\n",
      "\n",
      "Train Epoch: 188\t Learning rate: 0.0005\t Loss: 0.874929\t \n",
      "\n",
      "Test set: Average loss: 4.0638 \t MAE: 4.5159\n",
      "\n",
      "Train Epoch: 189\t Learning rate: 0.0005\t Loss: 1.043584\t \n",
      "\n",
      "Test set: Average loss: 4.0349 \t MAE: 4.5109\n",
      "\n",
      "Train Epoch: 190\t Learning rate: 0.0005\t Loss: 0.935190\t \n",
      "\n",
      "Test set: Average loss: 4.0865 \t MAE: 4.5353\n",
      "\n",
      "Train Epoch: 191\t Learning rate: 0.0005\t Loss: 0.694541\t \n",
      "\n",
      "Test set: Average loss: 4.3420 \t MAE: 4.8012\n",
      "\n",
      "Train Epoch: 192\t Learning rate: 0.0005\t Loss: 0.865203\t \n",
      "\n",
      "Test set: Average loss: 3.9585 \t MAE: 4.4290\n",
      "\n",
      "Train Epoch: 193\t Learning rate: 0.0005\t Loss: 0.858346\t \n",
      "\n",
      "Test set: Average loss: 4.1437 \t MAE: 4.6031\n",
      "\n",
      "Train Epoch: 194\t Learning rate: 0.0005\t Loss: 0.906553\t \n",
      "\n",
      "Test set: Average loss: 4.4437 \t MAE: 4.9088\n",
      "\n",
      "Train Epoch: 195\t Learning rate: 0.0005\t Loss: 0.836493\t \n",
      "\n",
      "Test set: Average loss: 4.1875 \t MAE: 4.6559\n",
      "\n",
      "Train Epoch: 196\t Learning rate: 0.0005\t Loss: 0.852588\t \n",
      "\n",
      "Test set: Average loss: 4.4612 \t MAE: 4.9229\n",
      "\n",
      "Train Epoch: 197\t Learning rate: 0.0005\t Loss: 0.905962\t \n",
      "\n",
      "Test set: Average loss: 4.4976 \t MAE: 4.9639\n",
      "\n",
      "Train Epoch: 198\t Learning rate: 0.0005\t Loss: 0.915486\t \n",
      "\n",
      "Test set: Average loss: 4.2601 \t MAE: 4.7137\n",
      "\n",
      "Train Epoch: 199\t Learning rate: 0.0005\t Loss: 0.840349\t \n",
      "\n",
      "Test set: Average loss: 4.0902 \t MAE: 4.5491\n",
      "\n",
      "Train Epoch: 200\t Learning rate: 0.0005\t Loss: 0.837486\t \n",
      "\n",
      "Test set: Average loss: 4.2122 \t MAE: 4.6674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(1, config['epochs']):\n",
    "    train(ep)\n",
    "    tloss = evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10  2]\n",
      " [ 2 21]]\n",
      "\n",
      "Test set: Average loss: 1.1024\n",
      "Calculating additional test metrics...\n",
      "Accuracy: 0.8857142857142857\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.8333333333333334\n",
      "F1-Score: 0.8333333333333334\n",
      "\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.102392554283142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = torch.load('/Users/apple/Downloads/depression/BiLSTM_elmo_128_0.83.pt')\n",
    "model = BiLSTM(config)\n",
    "model.load_state_dict(lstm_model.state_dict())\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 3.4328 \t MAE: 3.8846\n",
      "\n",
      "Saved as BiLSTM_reg_128_3.88.pt\n",
      "****************************************************************\n",
      "model saved: f1: 3.8845977783203125\tacc: 0.8374860261877378\n",
      "****************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.432814359664917"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = torch.load('/Users/apple/Downloads/depression/BiLSTM_reg_128_3.88.pt')\n",
    "model = BiLSTM(config)\n",
    "model.load_state_dict(lstm_model.state_dict())\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
