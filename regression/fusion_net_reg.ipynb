{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wave\n",
    "import librosa\n",
    "from python_speech_features import *\n",
    "import re\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prefix = '/Users/apple/Downloads/depression/'\n",
    "\n",
    "elmo = ElmoEmbedder()\n",
    "\n",
    "train_split_df = pd.read_csv(prefix+'train_split_Depression_AVEC2017 (1).csv')\n",
    "test_split_df = pd.read_csv(prefix+'dev_split_Depression_AVEC2017.csv')\n",
    "train_split_num = train_split_df[['Participant_ID']]['Participant_ID'].tolist()\n",
    "test_split_num = test_split_df[['Participant_ID']]['Participant_ID'].tolist()\n",
    "train_split_clabel = train_split_df[['PHQ8_Score']]['PHQ8_Score'].tolist()\n",
    "test_split_clabel = test_split_df[['PHQ8_Score']]['PHQ8_Score'].tolist()\n",
    "\n",
    "topics = []\n",
    "with open('/Users/apple/Downloads/depression/queries.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        topics.append(line.strip('\\n').strip())\n",
    "        \n",
    "\n",
    "def identify_topics(sentence):\n",
    "    if sentence in topics:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_features(number, audio_features, text_features, target, mode, targets):\n",
    "    transcript = pd.read_csv(prefix+'{0}_P/{0}_TRANSCRIPT.csv'.format(number), sep='\\t').fillna('')\n",
    "    \n",
    "    wavefile = wave.open(prefix+'{0}_P/{0}_AUDIO.wav'.format(number, 'r'))\n",
    "    sr = wavefile.getframerate()\n",
    "    nframes = wavefile.getnframes()\n",
    "    wave_data = np.frombuffer(wavefile.readframes(nframes), dtype=np.short)\n",
    "    \n",
    "    responses = []\n",
    "    response = ''\n",
    "    response_flag = False\n",
    "    start_time = 0\n",
    "    stop_time = 0\n",
    "    signal = []\n",
    "\n",
    "    global counter_train, counter_test\n",
    "\n",
    "    for t in transcript.itertuples():\n",
    "        # participant一句话结束\n",
    "        if getattr(t,'speaker') == 'Ellie':\n",
    "            if '(' in getattr(t,'value'):\n",
    "                content = re.findall(re.compile(r'[(](.*?)[)]', re.S), getattr(t,'value'))[0]\n",
    "            else:\n",
    "                content = getattr(t,'value').strip()\n",
    "            content = getattr(t,'value').strip()\n",
    "            if identify_topics(content):\n",
    "                response_flag = True\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response.strip())\n",
    "                response = ''\n",
    "            elif response_flag and len(content.split()) > 4:\n",
    "                response_flag = False\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response)\n",
    "                response = ''\n",
    "        elif getattr(t,'speaker') == 'Participant':\n",
    "            if 'scrubbed_entry' in getattr(t,'value'):\n",
    "                continue\n",
    "            elif response_flag:\n",
    "                response+=' '+getattr(t,'value').split('\\n')[0].strip()\n",
    "            start_time = int(getattr(t,'start_time')*sr)\n",
    "            stop_time = int(getattr(t,'stop_time')*sr)\n",
    "            signal = np.hstack((signal, wave_data[start_time:stop_time].astype(np.float)))\n",
    "            \n",
    "#     text features & audio_features\n",
    "    clip = sr*1*15\n",
    "    text_embeds = elmo.embed_sentence(responses).mean(0)\n",
    "    if target >= 10 and mode == 'train':\n",
    "        times = 3 if counter_train < 48 else 2\n",
    "        for i in range(times):\n",
    "            melspec = librosa.feature.melspectrogram(signal[clip*i:clip*(i+1)], n_mels=80,sr=sr)\n",
    "            audio_features.append(melspec)\n",
    "            text_features.append(text_embeds[i*10:(i+1)*10])\n",
    "            targets.append(target)\n",
    "            counter_train+=1\n",
    "    else:\n",
    "        melspec = librosa.feature.melspectrogram(signal[:clip], n_mels=80,sr=sr)\n",
    "        audio_features.append(melspec)\n",
    "        text_features.append(text_embeds[:10])\n",
    "        targets.append(target)\n",
    "        \n",
    "    print('{}_P feature done'.format(number))\n",
    "    \n",
    "    \n",
    "def extract_features_whole(number, audio_features, text_features, target, targets):\n",
    "    transcript = pd.read_csv(prefix+'{0}_P/{0}_TRANSCRIPT.csv'.format(number), sep='\\t').fillna('')\n",
    "    \n",
    "    wavefile = wave.open(prefix+'{0}_P/{0}_AUDIO.wav'.format(number, 'r'))\n",
    "    sr = wavefile.getframerate()\n",
    "    nframes = wavefile.getnframes()\n",
    "    wave_data = np.frombuffer(wavefile.readframes(nframes), dtype=np.short)\n",
    "    \n",
    "    responses = []\n",
    "    response = ''\n",
    "    response_flag = False\n",
    "    start_time = 0\n",
    "    stop_time = 0\n",
    "    signal = []\n",
    "\n",
    "    global counter_train, counter_test\n",
    "\n",
    "    for t in transcript.itertuples():\n",
    "        # participant一句话结束\n",
    "        if getattr(t,'speaker') == 'Ellie':\n",
    "            if '(' in getattr(t,'value'):\n",
    "                content = re.findall(re.compile(r'[(](.*?)[)]', re.S), getattr(t,'value'))[0]\n",
    "            else:\n",
    "                content = getattr(t,'value').strip()\n",
    "            content = getattr(t,'value').strip()\n",
    "            if identify_topics(content):\n",
    "                response_flag = True\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response.strip())\n",
    "                response = ''\n",
    "            elif response_flag and len(content.split()) > 4:\n",
    "                response_flag = False\n",
    "                if len(response) != 0:\n",
    "                    responses.append(response)\n",
    "                response = ''\n",
    "        elif getattr(t,'speaker') == 'Participant':\n",
    "            if 'scrubbed_entry' in getattr(t,'value'):\n",
    "                continue\n",
    "            elif response_flag:\n",
    "                response+=' '+getattr(t,'value').split('\\n')[0].strip()\n",
    "            start_time = int(getattr(t,'start_time')*sr)\n",
    "            stop_time = int(getattr(t,'stop_time')*sr)\n",
    "            signal = np.hstack((signal, wave_data[start_time:stop_time].astype(np.float)))\n",
    "            \n",
    "#     text features & audio_features\n",
    "    clip = sr*1*15\n",
    "    text_embedds = elmo.embed_sentence(responses).mean(0)\n",
    "    if target >= 10:\n",
    "        times = 3 if counter_train < 48 else 2\n",
    "        for i in range(times):\n",
    "            melspec = librosa.feature.melspectrogram(signal[clip*i:clip*(i+1)], n_mels=80,sr=sr)\n",
    "            audio_features.append(melspec)\n",
    "            text_features.append(text_embedds[i*10:(i+1)*10])\n",
    "            targets.append(target)\n",
    "            counter_train+=1\n",
    "    else:\n",
    "        melspec = librosa.feature.melspectrogram(signal[:clip], n_mels=80,sr=sr)\n",
    "        audio_features.append(melspec)\n",
    "        text_features.append(text_embedds[:10])\n",
    "        targets.append(target)\n",
    "        \n",
    "    print('{}_P feature done'.format(number))\n",
    "    \n",
    "    \n",
    "counter_train = 0\n",
    "    \n",
    "# training set\n",
    "features_train = []\n",
    "targets_train = []\n",
    "audio_features_train = []\n",
    "text_features_train = []\n",
    "\n",
    "# test set\n",
    "features_test = []\n",
    "ctargets_test = []\n",
    "audio_features_test = []\n",
    "text_features_test = []\n",
    "\n",
    "# # training set\n",
    "# for index in range(len(train_split_num)):\n",
    "#     extract_features(train_split_num[index], audio_features_train, text_features_train, \\\n",
    "#                      train_split_clabel[index], 'train', targets_train)\n",
    "\n",
    "# # test set\n",
    "# for index in range(len(test_split_num)):\n",
    "#     extract_features(test_split_num[index], audio_features_test, text_features_test, \\\n",
    "#                      test_split_clabel[index], 'test', ctargets_test)\n",
    "\n",
    "# # preprocess\n",
    "# audio_features_train = np.array(audio_features_train).astype('float32')\n",
    "# audio_features_test = np.array(audio_features_test).astype('float32')\n",
    "# audio_features_train = np.array([(X - X.min()) / (X.max() - X.min()) for X in audio_features_train])\n",
    "# audio_features_test = np.array([(X - X.min()) / (X.max() - X.min()) for X in audio_features_test])\n",
    "\n",
    "# for i in range(len(audio_features_train)):\n",
    "#     features_train.append([text_features_train[i], audio_features_train[i]])\n",
    "\n",
    "# for i in range(len(audio_features_test)):\n",
    "#     features_test.append([text_features_test[i], audio_features_test[i]])\n",
    "\n",
    "# print(np.shape(features_train), np.shape(features_test))\n",
    "\n",
    "audio_features_whole = []\n",
    "text_features_whole = []\n",
    "features_whole = []\n",
    "targets_whole = []\n",
    "whole_split_num = train_split_num + test_split_num\n",
    "whole_targets = train_split_clabel + test_split_clabel\n",
    "\n",
    "# for index in range(len(whole_split_num)):\n",
    "#     extract_features_whole(whole_split_num[index], audio_features_whole, text_features_whole, \\\n",
    "#                            whole_targets[index], targets_whole)\n",
    "    \n",
    "# # preprocess\n",
    "# audio_features_whole = np.array(audio_features_whole).astype('float32')\n",
    "# audio_features_whole = np.array([(X - X.min()) / (X.max() - X.min()) for X in audio_features_whole])\n",
    "\n",
    "# for i in range(len(audio_features_whole)):\n",
    "#     features_whole.append([text_features_whole[i], audio_features_whole[i]])\n",
    "    \n",
    "# print(np.shape(features_whole), np.shape(targets_whole))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Saving npz file locally...\")\n",
    "\n",
    "# np.savez(prefix+'data/fuse/train_samples_reg.npz', features_train)\n",
    "# np.savez(prefix+'data/fuse/test_samples_reg.npz', features_test)\n",
    "# np.savez(prefix+'data/fuse/train_labels_reg.npz', targets_train)\n",
    "# np.savez(prefix+'data/fuse/test_labels_reg.npz', ctargets_test)\n",
    "\n",
    "\n",
    "# np.savez(prefix+'data/fuse/whole_samples_reg.npz', features_whole)\n",
    "# np.savez(prefix+'data/fuse/whole_labels_reg.npz', targets_whole)\n",
    "\n",
    "features_train = np.load(prefix+'data/fuse/train_samples_reg.npz', allow_pickle=True)['arr_0']\n",
    "features_test = np.load(prefix+'data/fuse/test_samples_reg.npz', allow_pickle=True)['arr_0']\n",
    "targets_train = np.load(prefix+'data/fuse/train_labels_reg.npz', allow_pickle=True)['arr_0']\n",
    "ctargets_test = np.load(prefix+'data/fuse/test_labels_reg.npz', allow_pickle=True)['arr_0']\n",
    "\n",
    "X_train = np.array(features_train)\n",
    "X_test = np.array(features_test)\n",
    "Y_train = np.array(targets_train)\n",
    "Y_test = np.array(ctargets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # 双层lstm\n",
    "        self.lstm_net = nn.LSTM(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC层\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x : [len_seq, batch_size, embedding_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(x)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(1, 32, (1,7), 1)\n",
    "        self.conv2d_2 = nn.Conv2d(32, 32, (1,7), 1)\n",
    "        self.dense_1 = nn.Linear(29952, 128)\n",
    "        self.dense_2 = nn.Linear(128, 128)\n",
    "        self.dense_3 = nn.Linear(128, n_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv2d_1(x))\n",
    "        x = F.max_pool2d(x, (4, 3), (1, 3))\n",
    "        x = F.relu(self.conv2d_2(x))\n",
    "        x = F.max_pool2d(x, (1, 3), (1, 3))\n",
    "#         flatten in keras\n",
    "        x = x.permute((0, 2, 3, 1))\n",
    "        x = x.contiguous().view(-1, 29952)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "#         x = F.relu(self.dense_2(x))\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        output = F.softmax(self.dense_3(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename):\n",
    "    save_filename = '{}.pt'.format(filename)\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "    \n",
    "def standard_confusion_matrix(y_test, y_test_pred):\n",
    "    \"\"\"\n",
    "    Make confusion matrix with format:\n",
    "                  -----------\n",
    "                  | TP | FP |\n",
    "                  -----------\n",
    "                  | FN | TN |\n",
    "                  -----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray - 1D\n",
    "    y_pred : ndarray - 1D\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray - 2D\n",
    "    \"\"\"\n",
    "    [[tn, fp], [fn, tp]] = confusion_matrix(y_test, y_test_pred)\n",
    "    return np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "def model_performance(y_test, y_test_pred_proba):\n",
    "    \"\"\"\n",
    "    Evaluation metrics for network performance.\n",
    "    \"\"\"\n",
    "#     y_test_pred = y_test_pred_proba.data.max(1, keepdim=True)[1]\n",
    "    y_test_pred = y_test_pred_proba\n",
    "\n",
    "    # Computing confusion matrix for test dataset\n",
    "    conf_matrix = standard_confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    return y_test_pred, conf_matrix\n",
    "\n",
    "def plot_roc_curve(y_test, y_score):\n",
    "    \"\"\"\n",
    "    Plots ROC curve for final trained model. Code taken from:\n",
    "    https://vkolachalama.blogspot.com/2016/05/keras-implementation-of-mlp-neural.html\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(prefix+'images/BiLSTM_roc.png')\n",
    "    plt.close()\n",
    "\n",
    "class fusion_net(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_dims, rnn_layers, dropout, num_classes, kernel_height):\n",
    "        super(fusion_net, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.kernel_height = kernel_height\n",
    "        \n",
    "#         ============================= BiLSTM =================================\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 双层lstm\n",
    "        self.lstm_net = nn.LSTM(self.embed_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC层\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "        \n",
    "#         ============================= BiLSTM =================================\n",
    "\n",
    "#         ============================= cnn =============================\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(1, 32, (1,7), 1)\n",
    "        self.conv2d_2 = nn.Conv2d(32, 32, (1,7), 2)\n",
    "        self.dense_1 = nn.Linear(29952, 128)\n",
    "        self.dense_2 = nn.Linear(128, 128)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "#         ============================= cnn =============================\n",
    "\n",
    "#         ============================= last fc layer =============================\n",
    "        self.fc_final = nn.Sequential(\n",
    "            nn.Linear(256, self.num_classes),\n",
    "            nn.ReLU(),\n",
    "#             nn.Softmax(),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "    \n",
    "    def pretrained_feature(self, x):\n",
    "        with torch.no_grad():\n",
    "            x_text = []\n",
    "            x_audio = []\n",
    "            for ele in x:\n",
    "                x_text.append(ele[0])\n",
    "                x_audio.append(ele[1])\n",
    "            x_text, x_audio = Variable(torch.tensor(x_text).type(torch.FloatTensor), requires_grad=False),\\\n",
    "                                Variable(torch.tensor(x_audio).type(torch.FloatTensor), requires_grad=False)\n",
    "    #         ============================= BiLSTM =================================\n",
    "            # x : [len_seq, batch_size, embedding_dim]\n",
    "            x_text = x_text.permute(1, 0, 2)\n",
    "            output, (final_hidden_state, final_cell_state) = self.lstm_net(x_text)\n",
    "            # output : [batch_size, len_seq, n_hidden * 2]\n",
    "            output = output.permute(1, 0, 2)\n",
    "            # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "            final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "            # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "            # atten_out = self.attention_net(output, final_hidden_state)\n",
    "            atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "            text_feature = self.fc_out(atten_out)\n",
    "\n",
    "    #         ============================= BiLSTM =================================\n",
    "\n",
    "    #         ============================= cnn =============================\n",
    "            x_audio = x_audio.unsqueeze(1)\n",
    "            x_audio = F.relu(self.conv2d_1(x_audio))\n",
    "            x_audio = F.max_pool2d(x_audio, (4, 3), (1, 3))\n",
    "            x_audio = F.relu(self.conv2d_2(x_audio))\n",
    "            x_audio = F.max_pool2d(x_audio, (1, 3), (1, 3))\n",
    "    #         flatten in keras\n",
    "            x_audio = x_audio.permute((0, 2, 3, 1))\n",
    "            x_audio = x_audio.contiguous().view(-1, 29952)\n",
    "            x_audio = F.relu(self.dense_1(x_audio))\n",
    "#             x_audio = F.relu(self.dense_2(x_audio))\n",
    "            x_audio = self.dense_2(x_audio)\n",
    "            audio_feature = self.dropout(x_audio)\n",
    "#         ============================= cnn =============================\n",
    "        return (text_feature, audio_feature)\n",
    "        \n",
    "    def forward(self, x): \n",
    "#         tf, af = self.pretrained_feature(x)\n",
    "#         y = tf+af\n",
    "        output = self.fc_final(x)\n",
    "        return output\n",
    "    \n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, text_feature, audio_feature, target, model):\n",
    "        weight = model.fc_final[0].weight\n",
    "        bias = model.fc_final[0].bias\n",
    "        pred_text = F.linear(text_feature, weight[:, :config['hidden_dims']], bias).flatten()\n",
    "        pred_audio = F.linear(audio_feature, weight[:, config['hidden_dims']:], bias).flatten()\n",
    "        l = nn.L1Loss()\n",
    "        target = torch.tensor(target).type(torch.FloatTensor)\n",
    "#         return l(pred_text, target) + l(pred_audio, target) + \\\n",
    "#                 config['lambda']*torch.norm(weight[:, :config['hidden_dims']]) + \\\n",
    "#                 config['lambda']*torch.norm(weight[:, config['hidden_dims']:])  \n",
    "        return l(pred_text, target) + l(pred_audio, target)\n",
    "    \n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_classes': 1,\n",
    "    'dropout': 0.5,\n",
    "    'rnn_layers': 2,\n",
    "    'embedding_size': 1024,\n",
    "    'batch_size': 2,\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 6.25e-5,\n",
    "    'hidden_dims': 128,\n",
    "    'kernel_height': 35,\n",
    "    'cuda': False,\n",
    "    'lambda': 1e-2,\n",
    "}\n",
    "\n",
    "model = fusion_net(config['embedding_size'], config['hidden_dims'], \\\n",
    "                   config['rnn_layers'], config['dropout'], config['num_classes'], config['kernel_height'])\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = MyLoss()\n",
    "\n",
    "def train(epoch, X_train, Y_train):\n",
    "    global max_train_acc, train_acc\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    pred = np.array([])\n",
    "    for i in range(0, X_train.shape[0], config['batch_size']):\n",
    "        if i + config['batch_size'] > X_train.shape[0]:\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+config['batch_size'])], Y_train[i:(i+config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        # 将模型的参数梯度设置为0\n",
    "        optimizer.zero_grad()\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "#         concat_x = text_feature + audio_feature\n",
    "        output = model(concat_x)\n",
    "        pred = np.hstack((pred, output.data.flatten().numpy()))\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "#         loss = nn.L1Loss()(output, torch.tensor(y).type(torch.FloatTensor).view_as(output))\n",
    "        # 后向传播调整参数\n",
    "        loss.backward()\n",
    "        # 根据梯度更新网络参数\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        # loss.item()能够得到张量中的元素值\n",
    "        total_loss += loss.item()\n",
    "#     max_train_acc = correct\n",
    "    train_acc = correct\n",
    "    print('Train Epoch: {:2d}\\t Learning rate: {:.4f}\\t Loss: {:.6f}\\t MAE: {:.2f}\\n '.format(\n",
    "                epoch, config['learning_rate'], total_loss/batch_idx, mean_absolute_error(pred, Y_train)))\n",
    "\n",
    "\n",
    "def evaluate(model_name, X_test, Y_test, model):\n",
    "    model.eval()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    pred = np.array([])\n",
    "    global fold_idx, min_mae, maes\n",
    "    for i in range(0, X_test.shape[0], config['batch_size']):\n",
    "        if i + config['batch_size'] > X_test.shape[0]:\n",
    "            x, y = X_test[i:], Y_test[i:]\n",
    "        else:\n",
    "            x, y = X_test[i:(i+config['batch_size'])], Y_test[i:(i+config['batch_size'])]\n",
    "        if config['cuda']:\n",
    "            x, y = Variable(torch.from_numpy(x).type(torch.FloatTensor), requires_grad=True).cuda(), Variable(torch.from_numpy(y)).cuda()\n",
    "        text_feature, audio_feature = model.pretrained_feature(x)\n",
    "        with torch.no_grad():\n",
    "            concat_x = torch.cat((text_feature, audio_feature), dim=1)\n",
    "#             concat_x = text_feature + audio_feature\n",
    "            output = model(concat_x)\n",
    "        loss = criterion(text_feature, audio_feature, y, model)\n",
    "#         loss = nn.L1Loss()(output, torch.tensor(y).type(torch.FloatTensor).view_as(output))\n",
    "        pred = np.hstack((pred, output.data.flatten().numpy()))\n",
    "        total_loss += loss.item()\n",
    "        batch_idx+=1\n",
    "        \n",
    "    mae = mean_absolute_error(pred, Y_test)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\t MAE: {:.4f}\\n'.format(total_loss/batch_idx/2, mae))\n",
    "    \n",
    "#     y_test_pred, conf_matrix = model_performance(Y_test, pred[2:])\n",
    "    \n",
    "#     print('\\nTest set: Average loss: {:.4f}'.format(total_loss/len(X_test)))\n",
    "#     # custom evaluation metrics\n",
    "#     print('Calculating additional test metrics...')\n",
    "#     accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
    "#     precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
    "#     recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     print(\"Accuracy: {}\".format(accuracy))\n",
    "#     print(\"Precision: {}\".format(precision))\n",
    "#     print(\"Recall: {}\".format(recall))\n",
    "#     print(\"F1-Score: {}\\n\".format(f1_score))\n",
    "#     print('='*89)\n",
    "    \n",
    "    if min_mae >= mae and mae < 3.75:\n",
    "        min_mae = mae\n",
    "        save(model, '{}_{:.2f}'.format(model_name, mae))\n",
    "        print('*'*64)\n",
    "        print('model saved: mae: {}'.format(mae))\n",
    "        print('*'*64)\n",
    "#     if min_mae > mae:\n",
    "#         min_mae = mae\n",
    "#         maes[fold_idx] = mae\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = torch.load('/Users/apple/Downloads/depression/BiLSTM_reg_128_3.88.pt')\n",
    "cnn_model = CNN(1)\n",
    "cnn_model.load_state_dict(torch.load('/Users/apple/Downloads/depression/cnn_melspec.pt'))\n",
    "\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in cnn_model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", cnn_model.state_dict()[param_tensor].size())\n",
    "\n",
    "merge_ordereddict = lstm_model.state_dict().copy()\n",
    "merge_ordereddict.update(cnn_model.state_dict())\n",
    "\n",
    "model.load_state_dict(merge_ordereddict, strict=False)\n",
    "    \n",
    "if config['cuda']:\n",
    "    model = model.cuda()\n",
    "    \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# model.fc_final[0].weight.requires_grad = True\n",
    "# model.fc_final[0].bias.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1\t Learning rate: 0.0001\t Loss: 15.945302\t MAE: 7.64\n",
      " \n",
      "\n",
      "Test set: Average loss: 6.4131\t MAE: 6.5139\n",
      "\n",
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 15.191943\t MAE: 6.90\n",
      " \n",
      "\n",
      "Test set: Average loss: 6.1714\t MAE: 6.0099\n",
      "\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 14.615025\t MAE: 6.32\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.9428\t MAE: 5.5411\n",
      "\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 13.925727\t MAE: 5.62\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.7181\t MAE: 5.1119\n",
      "\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 13.140352\t MAE: 4.84\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.5050\t MAE: 4.6969\n",
      "\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 12.551642\t MAE: 4.25\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.3090\t MAE: 4.3621\n",
      "\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 11.744102\t MAE: 3.45\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.1247\t MAE: 4.1041\n",
      "\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 11.177486\t MAE: 2.92\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.9535\t MAE: 3.8833\n",
      "\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 10.232799\t MAE: 2.15\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.7962\t MAE: 3.8081\n",
      "\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 9.984821\t MAE: 2.06\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.6623\t MAE: 3.7982\n",
      "\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 9.233966\t MAE: 1.71\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.5584\t MAE: 3.8565\n",
      "\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 8.842617\t MAE: 1.63\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.4742\t MAE: 3.9553\n",
      "\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 8.347459\t MAE: 1.82\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.4139\t MAE: 4.0704\n",
      "\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 7.862373\t MAE: 1.98\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.3619\t MAE: 4.2124\n",
      "\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 7.455268\t MAE: 2.09\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.3106\t MAE: 4.3980\n",
      "\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 7.337877\t MAE: 2.27\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.2743\t MAE: 4.5659\n",
      "\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 7.070568\t MAE: 2.52\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.2431\t MAE: 4.7116\n",
      "\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 6.756092\t MAE: 2.92\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.2176\t MAE: 4.8543\n",
      "\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 6.176032\t MAE: 3.64\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1854\t MAE: 4.9508\n",
      "\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 6.153461\t MAE: 3.59\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1550\t MAE: 5.0686\n",
      "\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 6.064158\t MAE: 3.75\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1286\t MAE: 5.2212\n",
      "\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 5.712738\t MAE: 4.19\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1020\t MAE: 5.3566\n",
      "\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 5.321760\t MAE: 4.43\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.0780\t MAE: 5.4894\n",
      "\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 5.200335\t MAE: 4.81\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.0655\t MAE: 5.6479\n",
      "\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 4.859643\t MAE: 5.39\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.0445\t MAE: 5.7350\n",
      "\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 4.566400\t MAE: 5.39\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.0278\t MAE: 5.8694\n",
      "\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 4.927631\t MAE: 5.09\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.0123\t MAE: 5.9981\n",
      "\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 4.493985\t MAE: 5.66\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9970\t MAE: 6.1038\n",
      "\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 4.556439\t MAE: 5.44\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9856\t MAE: 6.2266\n",
      "\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 4.225974\t MAE: 5.95\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9684\t MAE: 6.3041\n",
      "\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 3.848111\t MAE: 6.40\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9510\t MAE: 6.3604\n",
      "\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 3.686821\t MAE: 6.43\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9443\t MAE: 6.4588\n",
      "\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 4.042261\t MAE: 6.46\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9367\t MAE: 6.5261\n",
      "\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 3.833993\t MAE: 6.91\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9292\t MAE: 6.5750\n",
      "\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 3.622858\t MAE: 6.64\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9196\t MAE: 6.6406\n",
      "\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 3.927679\t MAE: 6.74\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9104\t MAE: 6.7041\n",
      "\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 3.631351\t MAE: 7.08\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9079\t MAE: 6.7417\n",
      "\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 3.698635\t MAE: 6.85\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9031\t MAE: 6.7656\n",
      "\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 3.634480\t MAE: 7.04\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9000\t MAE: 6.8160\n",
      "\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 3.021760\t MAE: 7.51\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8990\t MAE: 6.8636\n",
      "\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 3.577245\t MAE: 6.90\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8957\t MAE: 6.9047\n",
      "\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 3.629328\t MAE: 7.25\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8932\t MAE: 6.9327\n",
      "\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 3.449248\t MAE: 7.55\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8858\t MAE: 6.9414\n",
      "\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 3.356167\t MAE: 7.39\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8776\t MAE: 6.9179\n",
      "\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 3.628908\t MAE: 7.37\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8780\t MAE: 6.9388\n",
      "\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 3.603449\t MAE: 7.26\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8830\t MAE: 7.0088\n",
      "\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 3.202220\t MAE: 7.56\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8841\t MAE: 7.0289\n",
      "\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 3.316691\t MAE: 7.72\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8797\t MAE: 7.0265\n",
      "\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 3.430311\t MAE: 7.52\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8838\t MAE: 7.0752\n",
      "\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 3.416905\t MAE: 7.69\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8834\t MAE: 7.0402\n",
      "\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 3.394169\t MAE: 7.31\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8834\t MAE: 7.0896\n",
      "\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 3.338101\t MAE: 7.42\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8911\t MAE: 7.1516\n",
      "\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 3.528209\t MAE: 7.71\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8889\t MAE: 7.1411\n",
      "\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 3.609814\t MAE: 7.45\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8956\t MAE: 7.1858\n",
      "\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 3.582710\t MAE: 7.74\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8931\t MAE: 7.1783\n",
      "\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 3.394482\t MAE: 7.90\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8815\t MAE: 7.1135\n",
      "\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 3.360765\t MAE: 7.62\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8810\t MAE: 7.1228\n",
      "\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 3.432635\t MAE: 8.07\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8681\t MAE: 7.0368\n",
      "\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 3.387634\t MAE: 7.51\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8702\t MAE: 7.0517\n",
      "\n",
      "Train Epoch: 60\t Learning rate: 0.0001\t Loss: 3.909332\t MAE: 7.81\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8698\t MAE: 7.0302\n",
      "\n",
      "Train Epoch: 61\t Learning rate: 0.0001\t Loss: 3.487082\t MAE: 7.39\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8715\t MAE: 7.0574\n",
      "\n",
      "Train Epoch: 62\t Learning rate: 0.0001\t Loss: 3.157486\t MAE: 7.39\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8707\t MAE: 7.0948\n",
      "\n",
      "Train Epoch: 63\t Learning rate: 0.0001\t Loss: 3.542231\t MAE: 7.53\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8737\t MAE: 7.0918\n",
      "\n",
      "Train Epoch: 64\t Learning rate: 0.0001\t Loss: 3.564008\t MAE: 7.67\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8697\t MAE: 7.0545\n",
      "\n",
      "Train Epoch: 65\t Learning rate: 0.0001\t Loss: 3.282348\t MAE: 7.48\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8724\t MAE: 7.0914\n",
      "\n",
      "Train Epoch: 66\t Learning rate: 0.0001\t Loss: 3.447939\t MAE: 7.64\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8779\t MAE: 7.1206\n",
      "\n",
      "Train Epoch: 67\t Learning rate: 0.0001\t Loss: 3.439540\t MAE: 7.82\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8729\t MAE: 7.0819\n",
      "\n",
      "Train Epoch: 68\t Learning rate: 0.0001\t Loss: 3.371727\t MAE: 7.44\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8717\t MAE: 7.1001\n",
      "\n",
      "Train Epoch: 69\t Learning rate: 0.0001\t Loss: 3.156762\t MAE: 7.43\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8768\t MAE: 7.1666\n",
      "\n",
      "Train Epoch: 70\t Learning rate: 0.0001\t Loss: 3.600368\t MAE: 7.79\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8744\t MAE: 7.1478\n",
      "\n",
      "Train Epoch: 71\t Learning rate: 0.0001\t Loss: 3.515149\t MAE: 7.45\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8796\t MAE: 7.1891\n",
      "\n",
      "Train Epoch: 72\t Learning rate: 0.0001\t Loss: 3.585879\t MAE: 7.80\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8833\t MAE: 7.1999\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73\t Learning rate: 0.0001\t Loss: 3.396107\t MAE: 7.49\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8798\t MAE: 7.1970\n",
      "\n",
      "Train Epoch: 74\t Learning rate: 0.0001\t Loss: 3.219485\t MAE: 7.98\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8776\t MAE: 7.2263\n",
      "\n",
      "Train Epoch: 75\t Learning rate: 0.0001\t Loss: 3.329794\t MAE: 7.99\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8692\t MAE: 7.1703\n",
      "\n",
      "Train Epoch: 76\t Learning rate: 0.0001\t Loss: 3.569724\t MAE: 7.54\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8704\t MAE: 7.1932\n",
      "\n",
      "Train Epoch: 77\t Learning rate: 0.0001\t Loss: 3.310316\t MAE: 7.87\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8671\t MAE: 7.1683\n",
      "\n",
      "Train Epoch: 78\t Learning rate: 0.0001\t Loss: 3.559334\t MAE: 7.65\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8582\t MAE: 7.1251\n",
      "\n",
      "Train Epoch: 79\t Learning rate: 0.0001\t Loss: 3.133425\t MAE: 7.77\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8639\t MAE: 7.1441\n",
      "\n",
      "Train Epoch: 80\t Learning rate: 0.0001\t Loss: 3.426604\t MAE: 8.10\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8644\t MAE: 7.0967\n",
      "\n",
      "Train Epoch: 81\t Learning rate: 0.0001\t Loss: 3.408102\t MAE: 7.63\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8646\t MAE: 7.1070\n",
      "\n",
      "Train Epoch: 82\t Learning rate: 0.0001\t Loss: 3.544641\t MAE: 7.91\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8624\t MAE: 7.0485\n",
      "\n",
      "Train Epoch: 83\t Learning rate: 0.0001\t Loss: 3.490253\t MAE: 7.52\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8662\t MAE: 7.0793\n",
      "\n",
      "Train Epoch: 84\t Learning rate: 0.0001\t Loss: 3.177535\t MAE: 7.61\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8700\t MAE: 7.1124\n",
      "\n",
      "Train Epoch: 85\t Learning rate: 0.0001\t Loss: 3.288186\t MAE: 7.92\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8691\t MAE: 7.0910\n",
      "\n",
      "Train Epoch: 86\t Learning rate: 0.0001\t Loss: 3.398840\t MAE: 7.67\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8595\t MAE: 7.0595\n",
      "\n",
      "Train Epoch: 87\t Learning rate: 0.0001\t Loss: 3.364793\t MAE: 7.73\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8561\t MAE: 7.0284\n",
      "\n",
      "Train Epoch: 88\t Learning rate: 0.0001\t Loss: 3.343212\t MAE: 7.22\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8590\t MAE: 7.0805\n",
      "\n",
      "Train Epoch: 89\t Learning rate: 0.0001\t Loss: 3.724082\t MAE: 7.59\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8583\t MAE: 7.0877\n",
      "\n",
      "Train Epoch: 90\t Learning rate: 0.0001\t Loss: 3.633918\t MAE: 7.29\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8549\t MAE: 7.0825\n",
      "\n",
      "Train Epoch: 91\t Learning rate: 0.0001\t Loss: 3.630354\t MAE: 7.59\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8618\t MAE: 7.1252\n",
      "\n",
      "Train Epoch: 92\t Learning rate: 0.0001\t Loss: 3.341214\t MAE: 7.71\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8564\t MAE: 7.0870\n",
      "\n",
      "Train Epoch: 93\t Learning rate: 0.0001\t Loss: 3.124119\t MAE: 7.37\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8625\t MAE: 7.1330\n",
      "\n",
      "Train Epoch: 94\t Learning rate: 0.0001\t Loss: 3.423077\t MAE: 7.62\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8715\t MAE: 7.1892\n",
      "\n",
      "Train Epoch: 95\t Learning rate: 0.0001\t Loss: 3.227518\t MAE: 7.62\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8738\t MAE: 7.2283\n",
      "\n",
      "Train Epoch: 96\t Learning rate: 0.0001\t Loss: 3.260963\t MAE: 7.56\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8715\t MAE: 7.2481\n",
      "\n",
      "Train Epoch: 97\t Learning rate: 0.0001\t Loss: 3.360247\t MAE: 7.77\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8730\t MAE: 7.2508\n",
      "\n",
      "Train Epoch: 98\t Learning rate: 0.0001\t Loss: 3.442613\t MAE: 7.92\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8798\t MAE: 7.2519\n",
      "\n",
      "Train Epoch: 99\t Learning rate: 0.0001\t Loss: 3.282196\t MAE: 7.64\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8820\t MAE: 7.2516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_f1 = -1\n",
    "max_acc = -1\n",
    "max_train_acc = -1\n",
    "min_mae = 100\n",
    "\n",
    "for ep in range(1, config['epochs']):\n",
    "    train(ep, X_train, Y_train)\n",
    "    tloss = evaluate('fuse_reg', X_test, Y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fuse_model = torch.load('/Users/apple/Downloads/depression/fuse_0.85.pt')\n",
    "tloss = evaluate('fuse', X_test, Y_test, fuse_model)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1\t Learning rate: 0.0001\t Loss: 17.441852\t MAE: 9.00\n",
      " \n",
      "\n",
      "Test set: Average loss: 16.8825\t MAE: 8.7609\n",
      "\n",
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 17.080365\t MAE: 8.64\n",
      " \n",
      "\n",
      "Test set: Average loss: 16.5166\t MAE: 8.3893\n",
      "\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 16.703621\t MAE: 8.26\n",
      " \n",
      "\n",
      "Test set: Average loss: 16.1613\t MAE: 8.0251\n",
      "\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 16.449851\t MAE: 8.00\n",
      " \n",
      "\n",
      "Test set: Average loss: 15.8069\t MAE: 7.6610\n",
      "\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 16.085894\t MAE: 7.63\n",
      " \n",
      "\n",
      "Test set: Average loss: 15.4582\t MAE: 7.3036\n",
      "\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 15.688275\t MAE: 7.23\n",
      " \n",
      "\n",
      "Test set: Average loss: 15.1002\t MAE: 6.9365\n",
      "\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 15.389376\t MAE: 6.94\n",
      " \n",
      "\n",
      "Test set: Average loss: 14.7430\t MAE: 6.5700\n",
      "\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 15.224189\t MAE: 6.77\n",
      " \n",
      "\n",
      "Test set: Average loss: 14.4024\t MAE: 6.2221\n",
      "\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 14.705560\t MAE: 6.25\n",
      " \n",
      "\n",
      "Test set: Average loss: 14.0505\t MAE: 5.8757\n",
      "\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 14.426508\t MAE: 5.98\n",
      " \n",
      "\n",
      "Test set: Average loss: 13.6967\t MAE: 5.5274\n",
      "\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 14.108157\t MAE: 5.65\n",
      " \n",
      "\n",
      "Test set: Average loss: 13.3431\t MAE: 5.1794\n",
      "\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 13.755353\t MAE: 5.29\n",
      " \n",
      "\n",
      "Test set: Average loss: 12.9912\t MAE: 4.8329\n",
      "\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 13.346429\t MAE: 4.90\n",
      " \n",
      "\n",
      "Test set: Average loss: 12.6338\t MAE: 4.4810\n",
      "\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 13.109369\t MAE: 4.67\n",
      " \n",
      "\n",
      "Test set: Average loss: 12.2797\t MAE: 4.1342\n",
      "\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 12.848923\t MAE: 4.44\n",
      " \n",
      "\n",
      "Test set: Average loss: 11.9412\t MAE: 3.7994\n",
      "\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 12.573522\t MAE: 4.15\n",
      " \n",
      "\n",
      "Test set: Average loss: 11.6099\t MAE: 3.4687\n",
      "\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 12.327918\t MAE: 3.92\n",
      " \n",
      "\n",
      "Test set: Average loss: 11.2907\t MAE: 3.1484\n",
      "\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 12.068094\t MAE: 3.66\n",
      " \n",
      "\n",
      "Test set: Average loss: 10.9734\t MAE: 2.8342\n",
      "\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 11.572874\t MAE: 3.24\n",
      " \n",
      "\n",
      "Test set: Average loss: 10.6530\t MAE: 2.5241\n",
      "\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 11.163184\t MAE: 2.86\n",
      " \n",
      "\n",
      "Test set: Average loss: 10.3254\t MAE: 2.2158\n",
      "\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 11.036268\t MAE: 2.76\n",
      " \n",
      "\n",
      "Test set: Average loss: 10.0104\t MAE: 1.9468\n",
      "\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 10.697159\t MAE: 2.51\n",
      " \n",
      "\n",
      "Test set: Average loss: 9.6910\t MAE: 1.7151\n",
      "\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 10.168467\t MAE: 2.29\n",
      " \n",
      "\n",
      "Test set: Average loss: 9.3659\t MAE: 1.5635\n",
      "\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 10.027455\t MAE: 2.16\n",
      " \n",
      "\n",
      "Test set: Average loss: 9.0482\t MAE: 1.5158\n",
      "\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 9.716768\t MAE: 2.12\n",
      " \n",
      "\n",
      "Test set: Average loss: 8.7321\t MAE: 1.5286\n",
      "\n",
      "Train Epoch: 26\t Learning rate: 0.0001\t Loss: 9.573299\t MAE: 2.11\n",
      " \n",
      "\n",
      "Test set: Average loss: 8.4231\t MAE: 1.6097\n",
      "\n",
      "Train Epoch: 27\t Learning rate: 0.0001\t Loss: 9.200572\t MAE: 2.18\n",
      " \n",
      "\n",
      "Test set: Average loss: 8.1181\t MAE: 1.7774\n",
      "\n",
      "Train Epoch: 28\t Learning rate: 0.0001\t Loss: 8.774662\t MAE: 2.30\n",
      " \n",
      "\n",
      "Test set: Average loss: 7.8195\t MAE: 2.0033\n",
      "\n",
      "Train Epoch: 29\t Learning rate: 0.0001\t Loss: 8.734874\t MAE: 2.31\n",
      " \n",
      "\n",
      "Test set: Average loss: 7.5243\t MAE: 2.2421\n",
      "\n",
      "Train Epoch: 30\t Learning rate: 0.0001\t Loss: 8.142350\t MAE: 2.56\n",
      " \n",
      "\n",
      "Test set: Average loss: 7.2215\t MAE: 2.5172\n",
      "\n",
      "Train Epoch: 31\t Learning rate: 0.0001\t Loss: 8.272336\t MAE: 2.63\n",
      " \n",
      "\n",
      "Test set: Average loss: 6.9471\t MAE: 2.7796\n",
      "\n",
      "Train Epoch: 32\t Learning rate: 0.0001\t Loss: 8.048263\t MAE: 2.84\n",
      " \n",
      "\n",
      "Test set: Average loss: 6.6658\t MAE: 3.0570\n",
      "\n",
      "Train Epoch: 33\t Learning rate: 0.0001\t Loss: 7.592144\t MAE: 3.18\n",
      " \n",
      "\n",
      "Test set: Average loss: 6.4029\t MAE: 3.3241\n",
      "\n",
      "Train Epoch: 34\t Learning rate: 0.0001\t Loss: 7.558122\t MAE: 3.22\n",
      " \n",
      "\n",
      "Test set: Average loss: 6.1555\t MAE: 3.6010\n",
      "\n",
      "Train Epoch: 35\t Learning rate: 0.0001\t Loss: 7.249080\t MAE: 3.55\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.9337\t MAE: 3.8718\n",
      "\n",
      "Train Epoch: 36\t Learning rate: 0.0001\t Loss: 7.005570\t MAE: 3.62\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.7146\t MAE: 4.1436\n",
      "\n",
      "Train Epoch: 37\t Learning rate: 0.0001\t Loss: 6.901254\t MAE: 3.87\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.5068\t MAE: 4.4118\n",
      "\n",
      "Train Epoch: 38\t Learning rate: 0.0001\t Loss: 6.634049\t MAE: 4.00\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.3192\t MAE: 4.6670\n",
      "\n",
      "Train Epoch: 39\t Learning rate: 0.0001\t Loss: 6.492435\t MAE: 4.11\n",
      " \n",
      "\n",
      "Test set: Average loss: 5.1303\t MAE: 4.9332\n",
      "\n",
      "Train Epoch: 40\t Learning rate: 0.0001\t Loss: 6.218497\t MAE: 4.43\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.9671\t MAE: 5.1601\n",
      "\n",
      "Train Epoch: 41\t Learning rate: 0.0001\t Loss: 6.028616\t MAE: 4.70\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.7995\t MAE: 5.4027\n",
      "\n",
      "Train Epoch: 42\t Learning rate: 0.0001\t Loss: 6.026868\t MAE: 5.01\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.6478\t MAE: 5.6201\n",
      "\n",
      "Train Epoch: 43\t Learning rate: 0.0001\t Loss: 5.941594\t MAE: 4.94\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.4930\t MAE: 5.8516\n",
      "\n",
      "Train Epoch: 44\t Learning rate: 0.0001\t Loss: 5.968868\t MAE: 5.14\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.3505\t MAE: 6.0581\n",
      "\n",
      "Train Epoch: 45\t Learning rate: 0.0001\t Loss: 5.812942\t MAE: 5.37\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.2272\t MAE: 6.2421\n",
      "\n",
      "Train Epoch: 46\t Learning rate: 0.0001\t Loss: 5.378947\t MAE: 5.78\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1195\t MAE: 6.3996\n",
      "\n",
      "Train Epoch: 47\t Learning rate: 0.0001\t Loss: 5.540126\t MAE: 5.70\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.0306\t MAE: 6.5247\n",
      "\n",
      "Train Epoch: 48\t Learning rate: 0.0001\t Loss: 5.629703\t MAE: 6.02\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.9432\t MAE: 6.6719\n",
      "\n",
      "Train Epoch: 49\t Learning rate: 0.0001\t Loss: 5.364712\t MAE: 5.79\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8735\t MAE: 6.7996\n",
      "\n",
      "Train Epoch: 50\t Learning rate: 0.0001\t Loss: 5.349440\t MAE: 5.94\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.8009\t MAE: 6.9318\n",
      "\n",
      "Train Epoch: 51\t Learning rate: 0.0001\t Loss: 5.224799\t MAE: 5.77\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.7284\t MAE: 7.0822\n",
      "\n",
      "Train Epoch: 52\t Learning rate: 0.0001\t Loss: 4.953100\t MAE: 6.06\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.6743\t MAE: 7.2130\n",
      "\n",
      "Train Epoch: 53\t Learning rate: 0.0001\t Loss: 5.165050\t MAE: 6.42\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.6190\t MAE: 7.3423\n",
      "\n",
      "Train Epoch: 54\t Learning rate: 0.0001\t Loss: 5.435852\t MAE: 6.12\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.5727\t MAE: 7.4473\n",
      "\n",
      "Train Epoch: 55\t Learning rate: 0.0001\t Loss: 4.999228\t MAE: 6.71\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.5464\t MAE: 7.5491\n",
      "\n",
      "Train Epoch: 56\t Learning rate: 0.0001\t Loss: 5.029483\t MAE: 6.67\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.5339\t MAE: 7.6325\n",
      "\n",
      "Train Epoch: 57\t Learning rate: 0.0001\t Loss: 5.139349\t MAE: 7.01\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.5158\t MAE: 7.6856\n",
      "\n",
      "Train Epoch: 58\t Learning rate: 0.0001\t Loss: 5.157308\t MAE: 6.93\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.5088\t MAE: 7.7516\n",
      "\n",
      "Train Epoch: 59\t Learning rate: 0.0001\t Loss: 4.883505\t MAE: 6.88\n",
      " \n",
      "\n",
      "Test set: Average loss: 3.4962\t MAE: 7.8049\n",
      "\n",
      "Train Epoch:  1\t Learning rate: 0.0001\t Loss: 4.785590\t MAE: 6.90\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1630\t MAE: 7.0442\n",
      "\n",
      "Train Epoch:  2\t Learning rate: 0.0001\t Loss: 4.768672\t MAE: 7.21\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1480\t MAE: 7.1220\n",
      "\n",
      "Train Epoch:  3\t Learning rate: 0.0001\t Loss: 4.536798\t MAE: 7.28\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1396\t MAE: 7.1644\n",
      "\n",
      "Train Epoch:  4\t Learning rate: 0.0001\t Loss: 4.940647\t MAE: 7.06\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1362\t MAE: 7.2142\n",
      "\n",
      "Train Epoch:  5\t Learning rate: 0.0001\t Loss: 4.711936\t MAE: 7.33\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1348\t MAE: 7.2504\n",
      "\n",
      "Train Epoch:  6\t Learning rate: 0.0001\t Loss: 4.720998\t MAE: 7.27\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1343\t MAE: 7.3237\n",
      "\n",
      "Train Epoch:  7\t Learning rate: 0.0001\t Loss: 4.635416\t MAE: 7.67\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1338\t MAE: 7.3397\n",
      "\n",
      "Train Epoch:  8\t Learning rate: 0.0001\t Loss: 4.779820\t MAE: 7.52\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1338\t MAE: 7.3642\n",
      "\n",
      "Train Epoch:  9\t Learning rate: 0.0001\t Loss: 4.648173\t MAE: 7.24\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1373\t MAE: 7.4107\n",
      "\n",
      "Train Epoch: 10\t Learning rate: 0.0001\t Loss: 5.028204\t MAE: 7.48\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1366\t MAE: 7.4031\n",
      "\n",
      "Train Epoch: 11\t Learning rate: 0.0001\t Loss: 4.510049\t MAE: 7.63\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1379\t MAE: 7.4159\n",
      "\n",
      "Train Epoch: 12\t Learning rate: 0.0001\t Loss: 4.709592\t MAE: 7.58\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1410\t MAE: 7.4478\n",
      "\n",
      "Train Epoch: 13\t Learning rate: 0.0001\t Loss: 4.545083\t MAE: 7.31\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 4.1445\t MAE: 7.4791\n",
      "\n",
      "Train Epoch: 14\t Learning rate: 0.0001\t Loss: 4.585653\t MAE: 7.49\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1520\t MAE: 7.5356\n",
      "\n",
      "Train Epoch: 15\t Learning rate: 0.0001\t Loss: 4.797094\t MAE: 7.48\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1522\t MAE: 7.5329\n",
      "\n",
      "Train Epoch: 16\t Learning rate: 0.0001\t Loss: 4.721240\t MAE: 7.79\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1545\t MAE: 7.5522\n",
      "\n",
      "Train Epoch: 17\t Learning rate: 0.0001\t Loss: 4.510887\t MAE: 8.07\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1524\t MAE: 7.5441\n",
      "\n",
      "Train Epoch: 18\t Learning rate: 0.0001\t Loss: 4.688483\t MAE: 7.68\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1569\t MAE: 7.5645\n",
      "\n",
      "Train Epoch: 19\t Learning rate: 0.0001\t Loss: 4.759270\t MAE: 7.69\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1582\t MAE: 7.5752\n",
      "\n",
      "Train Epoch: 20\t Learning rate: 0.0001\t Loss: 4.653477\t MAE: 8.00\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1597\t MAE: 7.5834\n",
      "\n",
      "Train Epoch: 21\t Learning rate: 0.0001\t Loss: 4.874827\t MAE: 7.59\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1604\t MAE: 7.5865\n",
      "\n",
      "Train Epoch: 22\t Learning rate: 0.0001\t Loss: 4.543398\t MAE: 8.13\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1544\t MAE: 7.5487\n",
      "\n",
      "Train Epoch: 23\t Learning rate: 0.0001\t Loss: 4.919513\t MAE: 7.70\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1548\t MAE: 7.5488\n",
      "\n",
      "Train Epoch: 24\t Learning rate: 0.0001\t Loss: 4.728646\t MAE: 7.78\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1560\t MAE: 7.5498\n",
      "\n",
      "Train Epoch: 25\t Learning rate: 0.0001\t Loss: 5.020917\t MAE: 7.83\n",
      " \n",
      "\n",
      "Test set: Average loss: 4.1548\t MAE: 7.5285\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-484-dae7d601acea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_whole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_whole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_whole\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_whole\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fuse_reg_cv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_whole\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_whole\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfold_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-481-f43115ad3b75>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, X_train, Y_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# 将模型的参数梯度设置为0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtext_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mconcat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-478-4171810459f5>\u001b[0m in \u001b[0;36mpretrained_feature\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m#         ============================= cnn =============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mx_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mx_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mx_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mx_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "features_whole = np.load(prefix+'data/fuse/whole_samples_reg.npz', allow_pickle=True)['arr_0']\n",
    "targets_whole = np.load(prefix+'data/fuse/whole_labels_reg.npz', allow_pickle=True)['arr_0']\n",
    "fold = 4\n",
    "fold_idx = 0\n",
    "kfold = StratifiedKFold(n_splits=fold, shuffle=True)\n",
    "maes = np.zeros(fold)\n",
    "\n",
    "for train_idx, test_idx in kfold.split(features_whole, targets_whole):\n",
    "    for ep in range(1, config['epochs']):\n",
    "        train(ep, features_whole[train_idx], targets_whole[train_idx])\n",
    "        tloss = evaluate('fuse_reg_cv', features_whole[test_idx], targets_whole[test_idx], model)\n",
    "    fold_idx += 1\n",
    "    min_mae = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945\n",
      "Precision: 0.9585360291882032\n",
      "Recall: 0.9299999999999999\n",
      "F1-Score: 0.9427670004171882\n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(np.mean(accs)))\n",
    "print(\"Precision: {}\".format(np.mean(precs)))\n",
    "print(\"Recall: {}\".format(np.mean(recs)))\n",
    "print(\"F1-Score: {}\\n\".format(np.mean(f1s)))\n",
    "print('='*89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9 , 0.98, 0.92, 0.98])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
